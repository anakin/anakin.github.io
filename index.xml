<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Anakin- 奔向NB的生活</title>
    <link>http://anakin.github.io/</link>
    <description>Recent content on Anakin- 奔向NB的生活</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Tue, 21 May 2019 18:11:37 +0800</lastBuildDate>
    
        <atom:link href="http://anakin.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>给Mac上的Fusion虚拟机设置固定ip地址</title>
      <link>http://anakin.github.io/post/fusion-static-ip/</link>
      <pubDate>Tue, 21 May 2019 18:11:37 +0800</pubDate>
      
      <guid>http://anakin.github.io/post/fusion-static-ip/</guid>
      
        <description>&lt;p&gt;因为最近需要安装k8s的本地测试环境，所以使用Mac上的Vmware Fusion安装了几台ubuntu系统的虚拟机，某次重启的时候发现ssh登录不上去了，打开虚拟机看了一下，发现是ip地址发生了变化，研究了半天，找到了解决的方法如下：&lt;br&gt;
在Mac的Terminal上编辑下面这个文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo vi /Library/Preferences/VMware\ Fusion/vmnet8/dhcpd.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后一行是下面的内容：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; ####### VMNET DHCP Configuration. End of &amp;quot;DO NOT MODIFY SECTION&amp;quot; #######
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在这行的下面，添加虚拟机的ip信息如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;host host1 {
    hardware ethernet 00:0c:29:dd:a5:67;
    fixed-address 192.168.32.131;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中:&lt;br&gt;
&lt;code&gt;host1&lt;/code&gt;是在Vmware Fusion中看到的虚拟机列表里的名称；&lt;br&gt;
&lt;code&gt;00:0c:29:dd:a5:67&lt;/code&gt;是这台虚拟机的网卡MAC地址，进入虚拟机的terminal里执行&lt;code&gt;ifconfig&lt;/code&gt;就可以找到&lt;br&gt;
&lt;code&gt;192.168.32.131&lt;/code&gt;是你要设置的固定ip的地址。&lt;/p&gt;

&lt;p&gt;设置好之后，执行下面的刷新操作:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo /Applications/VMware\ Fusion.app/Contents/Library/vmnet-cli --configure
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，&lt;code&gt;重新启动你的Mac&lt;/code&gt;，就可以生效了。&lt;/p&gt;

&lt;p&gt;亲测。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>手动搭建kubernetes集群（三）</title>
      <link>http://anakin.github.io/post/create-k8s-3/</link>
      <pubDate>Tue, 21 May 2019 16:46:07 +0800</pubDate>
      
      <guid>http://anakin.github.io/post/create-k8s-3/</guid>
      
        <description>

&lt;p&gt;本文是这个系列的第三篇文章，前两篇记录了搭建一个k8s集群的过程，但是之前搭建好的集群少了很重要的一个部分，就是安全相关的功能，包括认证、授权等机制。&lt;br&gt;
什么是认证，什么又是授权呢，可以简单的理解为，认证的目的是知道用户是谁，授权的目的是知道用户可以做什么。先认证，知道是谁，再授权知道能做什么。&lt;br&gt;
所谓的安全，主要是针对apiserver所说的，因为k8s通过apiserver提供RESTFUL接口，所以如果有人知道你的apiserver地址，就可以修改你的集群信息了。
首先了解一下相关的基础知识，包括SSL、JWT、RBAC等等。&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;ssl介绍&#34;&gt;SSL介绍&lt;/h2&gt;

&lt;p&gt;SSL是一个协议，https中的s，代表的就是SSL。
网上关于SSL的介绍很多也很详细，我这里只说一下我的理解。为了保证网络传输过程中的安全，传输的信息需要进行加密处理。 加密可以分为两类，对称加密和非对称加密。
1. &lt;strong&gt;对称加密&lt;/strong&gt;&lt;br&gt;
所谓对称加密， 就是说加密和解密的方法是对称的，加密方怎么加密，解密方就怎么反过来解密。举个例子：&lt;br&gt;
Client端通过对称的加密算法md5以及一个秘钥，加密一段信息，并将加密后的信息通过请求发送给server端：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;secret = md5(key+info)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;server端为了验证请求的来源合法性，采用同样的方式重新加密，并将结果和client端发来的加密结果对比，如果一致，就认为请求是合法的。
这个过程中，双方需要持有相同的key，并使用相同的加密方法。
2. &lt;strong&gt;非对称加密&lt;/strong&gt;&lt;br&gt;
理解了对称加密，很容易想到，非对称加密就是双方的操作不一致。以常用的RSA加密来举例子：&lt;br&gt;
server端事先会生成一对key，一个可以公开的叫公钥，一个不能公开的叫私钥。公钥的内容任何人可见，但是只能用来加密，私钥用来解密（具体的原理请参考相关资料，基本思想就是质数分解）。所以上面的请求过程就变成了client端用server端给的公钥，把请求信息加密，然后发送给server，server端在收到请求后，用自己的私钥就可以解密从而获得请求信息。&lt;br&gt;
3. &lt;strong&gt;对比&lt;/strong&gt;&lt;br&gt;
使用对称加密的时候，双方都需要知道key，这就存在key被泄露的风险，而非对称加密则不存在这个问题，公钥谁都可以看，私钥不存在传输给别人的过程，安全程度大大加强。&lt;br&gt;
但是非对称加密的问题是，运算速度比较慢，效率相对比较低。&lt;br&gt;
4. &lt;strong&gt;SSL&lt;/strong&gt;&lt;br&gt;
说了这么多，终于回到SSL了，SSL大概就是结合了上面说的对称和非对称加密，利用了两者的优势，具体的操作大概是这样的：&lt;br&gt;
非对称加密不是慢么？对称加密不是容易泄露key么？那好，用非对称的方式来传输对称加密使用的key，两个问题就都解决了。大致的工作流程如下：&lt;br&gt;
+ client向server发起请求，拿到server端的公钥
+ client用公钥把自己生成的key加密，然后发送给server
+ server用私钥解密，获得了client的key
+ 两端可以愉快的用这个key通过对称加密的方式通信了。&lt;br&gt;&lt;/p&gt;

&lt;p&gt;当然实际的请求流程比我这个要复杂的多，各位自行了解吧~~&lt;/p&gt;

&lt;h2 id=&#34;jwt介绍&#34;&gt;JWT介绍&lt;/h2&gt;

&lt;p&gt;JWT的全称是json web token，是一个标准，主要用于授权和信息交换。&lt;br&gt;
看名字就知道，这货就是个token，具体来说，是一个由“.”分割的三个部分组成的字符串，这三个部分分别是：
+ header
+ playload
+ signature&lt;/p&gt;

&lt;p&gt;看起来就是这样的aaaaaa.bbbb.cccc，这个字符串本身包含了一些信息，例如可以保存用户的ID等，这样服务端在接收到token之后，通过解密就直接拿到ID，不用再去数据库里查询了。token里同时还包括使用的签名算法等。具体的使用流程就是，server收到client请求的时候，用一个自己的secret使用某种加密算法生成一个这样的Token，然后发送给client，client获得token之后，每次请求都要在Authorization header里带上获得的token，header看起来是这样的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Authorization: Bearer &amp;lt;token&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;server端每次都会验证这个token是不是自己签发的那个有效的token，从而实现无状态http服务的状态，是不是感觉作用和session有些类似？其实还是有些差异的，比如： session存储在服务端，JWT存储在客户端。&lt;/p&gt;

&lt;h2 id=&#34;rbac介绍&#34;&gt;RBAC介绍&lt;/h2&gt;

&lt;p&gt;RBAC的全称是Role-Based Access Control，基于角色的访问控制。&lt;br&gt;
下面是我粗浅的理解：&lt;br&gt;
把系统的操作权限拆分成一个个的小单位，多个小单位赋予某个角色，然后让用户属于某个角色，这样就可以灵活的控制用户对系统的访问控制了。还是举个例子：&lt;br&gt;
某个管理后台里有很多功能，比如用户管理、订单管理、商品管理、用户留言管理，然后定义几个角色：超级管理员有所有权限，运营管理员有用户管理和留言管理权限，财务管理员有订单管理权限。ok，这样一个用户进来这个后台的时候，根据需要给他赋予某个角色，他就有了对应的管理权限，一个角色可以有多个用户，多个角色可以有相同的权限，也可以随时调整角色和权限的关系，非常灵活。不知道我说清楚了没有。具体的还请查阅相关文档。&lt;/p&gt;

&lt;h2 id=&#34;kubernetes的认证和授权&#34;&gt;kubernetes的认证和授权&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;认证&lt;/strong&gt;&lt;br&gt;
kubernetes支持三种方式的认证：&lt;br&gt;&lt;/li&gt;
&lt;li&gt;HTTPS证书认证：基于CA根证书签名的双向数字证书认证方式，用到的就是前面说的SSL；&lt;/li&gt;
&lt;li&gt;HTTP Token认证：通过一个Token来识别合法用户，可以是普通token也可以是前面说过的JWT token；&lt;/li&gt;

&lt;li&gt;&lt;p&gt;HTTP Base认证：通过用户名+密码的方式认证；&lt;br&gt;
apiserver支持设置一种或多种认证方式，如果设置了多种，那么通过其中任何一种，都认为是认证成功了。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;授权&lt;/strong&gt;&lt;br&gt;
apiserver支持多种授权模式，例如Node,RBAC,Webhook等，可以在apiserver启动的时候指定授权模式，同样也可以指定一种或者多种，如果指定了多种，通过其中的某一种就认为是授权成功了，和认证类似。&lt;br&gt;
客户端访问apiserver的时候，发起的http request中带有各种属性，例如user,group,path等，授权过程就是将这些属性与配置好的授权模式去比较，从而判断是否可以授权对应的操作。&lt;br&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;

&lt;p&gt;絮絮叨叨的总算写完了，说的再多，都不如撸起袖子加油干，接下来就在之前搭建好的基础版集群环境里去试验一下吧~~~&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>手动搭建kubernetes集群（二）</title>
      <link>http://anakin.github.io/post/create-k8s-2/</link>
      <pubDate>Mon, 20 May 2019 15:18:19 +0800</pubDate>
      
      <guid>http://anakin.github.io/post/create-k8s-2/</guid>
      
        <description>

&lt;p&gt;根据前文准备好的环境，我们现在来一步步的搭建一个基础的k8s集群&lt;br&gt;
&lt;code&gt;注意，这里的配置信息都是按照我自己的虚拟环境来写的。&lt;/code&gt;&lt;br&gt;
把server01作为master节点，server02和server03作为worker节点&lt;br&gt;
各个节点需要配置的服务和命令如下：&lt;br&gt;
master节点上需要部署的服务包括：&lt;code&gt;etcd服务&lt;/code&gt;、&lt;code&gt;APIServer服务&lt;/code&gt;、&lt;code&gt;Scheduler服务&lt;/code&gt;、&lt;code&gt;ControllerManager服务&lt;/code&gt;、&lt;code&gt;CalicoNode服务&lt;/code&gt;、&lt;code&gt;kube-proxy服务&lt;/code&gt;、&lt;code&gt;kubectl命令&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;worker节点上需要部署的服务包括：&lt;code&gt;CalicoNode服务&lt;/code&gt;、&lt;code&gt;kubelet服务&lt;/code&gt;、&lt;code&gt;kube-proxy服务&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&#34;步骤1-准备文件&#34;&gt;步骤1：准备文件&lt;/h2&gt;

&lt;p&gt;安装k8s集群有几种方式可以选择，比如容器化的方式，比如用kubeadmin的方式，这次我们打算尝试的是使用二进制文件的方式。&lt;br&gt;
1. 登录到master虚拟机上（server01），从github上下载安装文件的压缩包，我们使用的是1.13.6版本：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    wget https://github.com/kubernetes/kubernetes/releases/download/v1.13.6/kubernetes.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;解压缩&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tar zxvf kubernetes.tar.gz
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;下载文件，进入刚刚解压好的文件夹&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd kubernetes
./cluster/get-kube-binaries.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个步骤因为涉及到从官网下载文件，由于墙的原因会非常缓慢或者失败，请自行上网解决。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;步骤2-master环境部署&#34;&gt;步骤2：master环境部署&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;etcd部署：&lt;br&gt;
编写etcd服务的启动配置文件etcd.service，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target
Documentation=https://github.com/coreos

[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
ExecStart=/home/anakin/bin/etcd \
--name=192.168.32.131 \
--listen-client-urls=http://192.168.32.131:2379,http://127.0.0.1:2379 \
--advertise-client-urls=http://192.168.32.131:2379 \
--data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;然后执行以下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cp etcd.service /lib/systemd/system/
systemctl enable etcd.service
mkdir -p /var/lib/etcd
service etcd start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果没有问题的话，etcd服务应该已经跑起来了。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;APIServer部署&lt;br&gt;
编写kube-apiserver.service文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target
[Service]
ExecStart=/home/anakin/bin/kube-apiserver \
--admission-control=NamespaceLifecycle,LimitRanger,DefaultStorageClass,ResourceQuota,NodeRestriction \
--insecure-bind-address=0.0.0.0 \
--kubelet-https=false \
--service-cluster-ip-range=10.68.0.0/16 \
--service-node-port-range=20000-40000 \
--etcd-servers=http://192.168.32.131:2379 \
--enable-swagger-ui=true \
--allow-privileged=true \
--audit-log-maxage=30 \
--audit-log-maxbackup=3 \
--audit-log-maxsize=100 \
--audit-log-path=/var/lib/audit.log \
--event-ttl=1h \
--v=2
Restart=on-failure
RestartSec=5
Type=notify
LimitNOFILE=65536
[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后执行和启动etcd类似的命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cp kube-apiserver.service /lib/systemd/system/
systemctl enable kube-apiserver.service
service kube-apiserver start
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ControllerManager部署&lt;br&gt;
编写kube-controller-manager.service文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
[Service]
ExecStart=/home/anakin/bin/kube-controller-manager \
--address=127.0.0.1 \
--master=http://127.0.0.1:8080 \
--allocate-node-cidrs=true \
--service-cluster-ip-range=10.68.0.0/16 \
--cluster-cidr=172.20.0.0/16 \
--cluster-name=kubernetes \
--leader-elect=true \
--cluster-signing-cert-file= \
--cluster-signing-key-file= \
--v=2
Restart=on-failure
RestartSec=5
[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，同样的方式启动服务:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cp kube-controller-manager.service /lib/systemd/system/
systemctl enable kube-controller-manager.service
service kube-controller-manager start
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Scheduler部署&lt;br&gt;
编写kube-scheduler.service文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
ExecStart=/home/anakin/bin/kube-scheduler \
--address=127.0.0.1 \
--master=http://127.0.0.1:8080 \
--leader-elect=true \
--v=2
Restart=on-failure
RestartSec=5
[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;然后，继续上面的启动方式:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cp kube-scheduler.service /lib/systemd/system/
systemctl enable kube-scheduler.service
service kube-scheduler start
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;CalicoNode部署
编写kube-calico.service文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Unit]
Description=calico node
After=docker.service
Requires=docker.service

[Service]
User=root
PermissionsStartOnly=true
ExecStart=/usr/bin/docker run --net=host --privileged --name=calico-node \
-e ETCD_ENDPOINTS=http://192.168.32.131:2379 \
-e CALICO_LIBNETWORK_ENABLED=true \
-e CALICO_NETWORKING_BACKEND=bird \
-e CALICO_DISABLE_FILE_LOGGING=true \
-e CALICO_IPV4POOL_CIDR=172.20.0.0/16 \
-e CALICO_IPV4POOL_IPIP=off \
-e FELIX_DEFAULTENDPOINTTOHOSTACTION=ACCEPT \
-e FELIX_IPV6SUPPORT=false \
-e FELIX_LOGSEVERITYSCREEN=info \
-e FELIX_IPINIPMTU=1440 \
-e FELIX_HEALTHENABLED=true \
-e IP=192.168.32.131 \
-v /var/run/calico:/var/run/calico \
-v /lib/modules:/lib/modules \
-v /run/docker/plugins:/run/docker/plugins \
-v /var/run/docker.sock:/var/run/docker.sock \
-v /var/log/calico:/var/log/calico \
registry.anakin.sun.com/k8s/calico-node:v2.6.2
ExecStop=/usr/bin/docker rm -f calico-node
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;同样的方式启动：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cp kube-calico.service /lib/systemd/system/
systemctl enable kube-calico.service
service kube-calico start
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;kubectl命令配置
执行以下命令(root)：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config set-cluster kubernetes  --server=http://192.168.1.102:8080
kubectl config set-context kubernetes --cluster=kubernetes
kubectl config use-context kubernetes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果有问题，可以手动修改配置文件：~/.kube/config&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;kube-proxy服务部署
编写kube-proxy.service文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target
[Service]
WorkingDirectory=/var/lib/kube-proxy
ExecStart=/home/anakin/bin/kube-proxy \
--bind-address=192.168.32.131 \
--hostname-override=192.168.32.131 \
--kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \
--logtostderr=true \
--v=2
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;编写kube-proxy.kubeconfig文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
clusters:
- cluster:
    server: http://192.168.32.131:8080
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
  name: default
current-context: default
kind: Config
preferences: {}
users: []
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后执行如下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p /var/lib/kube-proxy
cp kube-proxy.service /lib/systemd/system/
cp kube-proxy.kubeconfig /etc/kubernetes/
systemctl enable kube-proxy.service
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;OK,至此，master节点应该已经配置完成了。&lt;/p&gt;

&lt;h2 id=&#34;步骤3-worker环境部署&#34;&gt;步骤3：worker环境部署&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;CalicoNode部署&lt;br&gt;
参考master部分的内容&lt;/li&gt;

&lt;li&gt;&lt;p&gt;kube-proxy服务部署&lt;br&gt;
参考master部分的内容&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;kubelet服务配置&lt;br&gt;
编写kubelet.service文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=docker.service
Requires=docker.service

[Service]
WorkingDirectory=/var/lib/kubelet
ExecStart=/home/anakin/bin/kubelet \
--address=192.168.32.131 \
--hostname-override=192.168.32.131 \
--pod-infra-container-image=registry.anakin.sun.com/imooc/pause-amd64:3.0 \
--kubeconfig=/etc/kubernetes/kubelet.kubeconfig \
--network-plugin=cni \
--cni-conf-dir=/etc/cni/net.d \
--cni-bin-dir=/home/anakin/bin \
--cluster-dns=10.68.0.2 \
--cluster-domain=cluster.local. \
--allow-privileged=true \
--fail-swap-on=false \
--logtostderr=true \
--v=2
ExecStartPost=/sbin/iptables -A INPUT -s 10.0.0.0/8 -p tcp --dport 4194 -j ACCEPT
ExecStartPost=/sbin/iptables -A INPUT -s 172.16.0.0/12 -p tcp --dport 4194 -j ACCEPT
ExecStartPost=/sbin/iptables -A INPUT -s 192.168.0.0/16 -p tcp --dport 4194 -j ACCEPT
ExecStartPost=/sbin/iptables -A INPUT -p tcp --dport 4194 -j DROP
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;编写kubelet.kubeconfig文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: http://192.168.32.131:8080
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: &amp;quot;&amp;quot;
  name: system:node:kube-master
current-context: system:node:kube-master
kind: Config
preferences: {}
users: []
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编写10-calico.conf文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    &amp;quot;name&amp;quot;: &amp;quot;calico-k8s-network&amp;quot;,
    &amp;quot;cniVersion&amp;quot;: &amp;quot;0.1.0&amp;quot;,
    &amp;quot;type&amp;quot;: &amp;quot;calico&amp;quot;,
    &amp;quot;etcd_endpoints&amp;quot;: &amp;quot;http://192.168.32.131:2379&amp;quot;,
    &amp;quot;log_level&amp;quot;: &amp;quot;info&amp;quot;,
    &amp;quot;ipam&amp;quot;: {
        &amp;quot;type&amp;quot;: &amp;quot;calico-ipam&amp;quot;
    },
    &amp;quot;kubernetes&amp;quot;: {
        &amp;quot;k8s_api_root&amp;quot;: &amp;quot;http://192.168.32.131:8080&amp;quot;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，执行如下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p /var/lib/kubelet
mkdir -p /etc/kubernetes
mkdir -p /etc/cni/net.d

cp kubelet.service /lib/systemd/system/
cp kubelet.kubeconfig /etc/kubernetes/
cp 10-calico.conf /etc/cni/net.d/

systemctl enable kubelet.service
service kubelet start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;至此，整个集群应该已经搭建好了，如果中间遇到什么问题，可以通过查看系统日志，或者google解决。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>手动搭建kubernetes集群（一）</title>
      <link>http://anakin.github.io/post/create-k8s/</link>
      <pubDate>Mon, 20 May 2019 10:57:16 +0800</pubDate>
      
      <guid>http://anakin.github.io/post/create-k8s/</guid>
      
        <description>

&lt;p&gt;最近在看有关k8s的一些知识，想手动搭建一套环境来体验一下，本文详细记录一下搭建的过程。&lt;/p&gt;

&lt;h2 id=&#34;环境&#34;&gt;环境&lt;/h2&gt;

&lt;p&gt;三台Ubuntu系统的虚拟机，其中一台作为master，另外两台作为worker节点&lt;/p&gt;

&lt;h2 id=&#34;步骤1-安装虚拟机&#34;&gt;步骤1:安装虚拟机&lt;/h2&gt;

&lt;p&gt;我的笔记本是一台Macbook Pro，首选安装一个虚拟机软件Vmware Fusion，过程略。&lt;br&gt;
1. 下载好Ubuntu镜像，我选择的是&lt;code&gt;19.04&lt;/code&gt;版本。&lt;br&gt;
2. 打开Fusion，选择New，然后选择“Install from disk or image”,continue
3. 设置好用户名和密码，中间还可以修改使用的硬盘空间等等，这个过程就不详述了。
4. 安装好之后，进入系统，找到“terminal”，安装net-tools和ssh server
    &lt;code&gt;
    sudo apt install net-tools ssh
&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&#34;步骤2-安装docker&#34;&gt;步骤2：安装docker&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;更新包列表：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt update
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;让apt支持https方式&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt install apt-transport-https ca-certificates curl software-properties-common
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;添加GPG密钥&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;添加docker源&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo add-apt-repository &amp;quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;再次更新apt&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt update
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;确保从Docker repo安装而不是默认的Ubuntu repo&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apt-cache policy docker-ce
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;开始安装docker&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt install docker-ce
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;接受所有ip的数据包转发&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vi /lib/systemd/system/docker.service
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;找到ExecStart，在这上面加入下面一行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ExecStartPost=/sbin/iptables -I FORWARD -s 0.0.0.0/0 -j ACCEPT
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;重启&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo systemctl daemon-reload
sudo service docker restart
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;步骤3-系统设置&#34;&gt;步骤3：系统设置&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;关闭防火墙&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ufw disable
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;设置系统转发参数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/sysctl.d/k8s.conf
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.conf.all.rp_filter = 0
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;sysctl -p /etc/sysctl.d/k8s.conf
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;安装ntp服务，同步时钟&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt install ntp
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;步骤4-复制虚拟机&#34;&gt;步骤4：复制虚拟机&lt;/h2&gt;

&lt;p&gt;使用fusion的复制功能，复制出另外两台虚拟机&lt;br&gt;
关掉刚装好的虚拟机，选择Fusion菜单Virtual Machine下面的Create Full Clone，Fusion就会clone出一个一模一样的虚拟机出来，是不是很方便&lt;/p&gt;

&lt;h2 id=&#34;步骤5-设置免登陆和hosts文件&#34;&gt;步骤5：设置免登陆和hosts文件&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;打开三台虚拟机，进入terminal，执行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ifconfig
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;找到各自的ip地址&lt;br&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;打开Mac的terminal，输入下面的命令生成ssh公钥&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ssh-keygen -t rsa
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;将公钥分别拷贝到三台虚拟机上&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scp .ssh/id_rsa.pub user@xxx.xxx.xxx.xxx:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;user是虚拟机上的用户名，xxx代表各自的ip地址&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;在虚拟机上将公钥追加到.ssh/authorized_keys文件，并修改权限&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat id_rsa.pub &amp;gt;&amp;gt; .ssh/authorized_keys
chmod 600 authorized_keys
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;分别修改每台虚拟机上的hosts文件，用vim编辑器打开/etc/hosts,添加三台虚拟机的hosts信息，下面是我自己的host信息：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;192.168.32.131 server01
192.168.32.132 server02
192.168.32.133 server03
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;ok，到现在为止，基本的安装环境应该是准备好了。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>Etcd实现MVCC的原理</title>
      <link>http://anakin.github.io/post/etcd-mvcc/</link>
      <pubDate>Sun, 19 May 2019 22:52:01 +0800</pubDate>
      
      <guid>http://anakin.github.io/post/etcd-mvcc/</guid>
      
        <description>

&lt;p&gt;etcd满足的是CAP理论中的CP，实现了最终的强一致，使用Raft协议，Quorum机制（大多数同意原则）,&lt;/p&gt;

&lt;h2 id=&#34;mvcc的意思&#34;&gt;MVCC的意思&lt;/h2&gt;

&lt;p&gt;Multi-Version Concurrency Control 多版本并发控制，目的是为了实现并发访问&lt;/p&gt;

&lt;h2 id=&#34;实现原理&#34;&gt;实现原理&lt;/h2&gt;

&lt;p&gt;在etcd中，MVCC是如何实现的呢，先来看一下相关的代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type revision struct {
main int64
sub int64
} 

type generation struct {
ver     int64
created revision 
revs    []revision
} 

type keyIndex struct {
key         []byte
modified    revision 
generations []generation
} 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;根据代码，可以看出：
* 每个tx事务有唯一事务ID，在etcd中叫做main ID，全局递增不重复。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;一个tx可以包含多个修改操作（put和delete），每一个操作叫做一个revision（修订），共享同一个main ID。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;一个tx内连续的多个修改操作会被从0递增编号，这个编号叫做sub ID。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;每个revision由（main ID，sub ID）唯一标识。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;多个版本的修改历史，保存在generations中&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;每一次操作行为都被单独记录下来，用户value保存到bbolt中。&lt;/p&gt;

&lt;p&gt;在bbolt中，每个revision将作为key，即序列化（revision.main+revision.sub）作为key。因此，我们先通过内存btree在keyIndex.generations[0].revs中找到最后一条revision，即可去bbolt中读取对应的数据。&lt;/p&gt;

&lt;p&gt;相应的，etcd支持按key前缀查询，其实也就是遍历btree的同时根据revision去bbolt中获取用户的value。&lt;/p&gt;

&lt;p&gt;总结一下就是，内存btree维护的是用户key =&amp;gt; keyIndex的映射，keyIndex内维护多版本的revision信息，而revision可以映射到磁盘bbolt中的用户value。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>Redis持久化</title>
      <link>http://anakin.github.io/post/redis-rdb-aof/</link>
      <pubDate>Fri, 17 May 2019 13:22:22 +0800</pubDate>
      
      <guid>http://anakin.github.io/post/redis-rdb-aof/</guid>
      
        <description>

&lt;p&gt;redis的持久化有两种方式，RDB和AOF
 ## RDB:
在指定的时间间隔内，执行指定次数的写操作，则会将内存中的数据写入到磁盘中。即在指定目录下生成一个dump.rdb文件。Redis 重启会通过加载dump.rdb文件恢复数据。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;配置方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;save 900 1
save 300 10
save 60 10000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;意思是， 900秒内有1个更改，300秒内有10个更改以及60秒内有10000个更改，则将内存中的数据快照写入磁盘。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;恢复方法&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;将dump.rdb 文件拷贝到redis的安装目录的bin目录下，重启redis服务即可&lt;/p&gt;

&lt;h2 id=&#34;aof&#34;&gt;AOF&lt;/h2&gt;

&lt;p&gt;采用日志的形式来记录每个写操作，并追加到文件中。Redis 重启的会根据日志文件的内容将写指令从前到后执行一次以完成数据的恢复工作。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;配置方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;appendonly yes
appendfilename &amp;quot;appendonly.aof&amp;quot;
#指定更新条件
# appendfsync always
appendfsync everysec
# appendfsync no
#配置重写触发机制
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;恢复方法&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;将appendonly.aof 文件拷贝到redis的安装目录的bin目录下，重启redis服务即可。如果因为某些原因导致appendonly.aof 文件格式异常，从而导致数据还原失败，可以通过命令redis-check-aof &amp;ndash;fix appendonly.aof 进行修复&lt;/p&gt;

&lt;h2 id=&#34;区别&#34;&gt;区别&lt;/h2&gt;

&lt;p&gt;RDB通过fork的方式进行处理，性能更好
AOF备份所有的操作，数据更完整，但是效率略差，文件相对较大
实际环境中，可以两者同时使用&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>Golang利用redis实现分布式锁</title>
      <link>http://anakin.github.io/post/golang-redis-lock/</link>
      <pubDate>Thu, 16 May 2019 20:11:09 +0800</pubDate>
      
      <guid>http://anakin.github.io/post/golang-redis-lock/</guid>
      
        <description>

&lt;h2 id=&#34;原理&#34;&gt;原理&lt;/h2&gt;

&lt;p&gt;使用SETNX命令(SET if Not eXists)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SETNX key value
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将 key 的值设为 value，当且仅当 key 不存在。若给定的 key 已经存在，则 SETNX 不做任何动作。&lt;/p&gt;

&lt;p&gt;设置成功，返回 1 。&lt;/p&gt;

&lt;p&gt;设置失败，返回 0 。&lt;/p&gt;

&lt;p&gt;为防止获取锁之后，忘记删除，成功后再设置一个过期时间&lt;/p&gt;

&lt;p&gt;以上就是利用redis实现分布式锁的原理&lt;/p&gt;

&lt;h2 id=&#34;代码&#34;&gt;代码&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
	&amp;quot;fmt&amp;quot;
	&amp;quot;github.com/gomodule/redigo/redis&amp;quot;
	&amp;quot;log&amp;quot;
	&amp;quot;time&amp;quot;
)

type Lock struct {
	resource string
	token    string
	conn     redis.Conn
	timeout  int
}

func (lock *Lock) tryLock() (ok bool, err error) {
	_, err = redis.String(lock.conn.Do(&amp;quot;SET&amp;quot;, lock.key(), lock.token, &amp;quot;EX&amp;quot;, int(lock.timeout), &amp;quot;NX&amp;quot;))
	if err == redis.ErrNil {
		return false, nil
	}
	if err != nil {
		return false, err
	}
	return true, nil
}
func (lock *Lock) Unlock() (err error) {
	_, err = lock.conn.Do(&amp;quot;del&amp;quot;, lock.key())
	return
}

func (lock *Lock) key() string {
	return fmt.Sprintf(&amp;quot;redislock:%s&amp;quot;, lock.resource)
}

func (lock *Lock) AddTimeout(ex_time int64) (ok bool, err error) {
	ttl_time, err := redis.Int64(lock.conn.Do(&amp;quot;TTL&amp;quot;, lock.key()))
	fmt.Println(ttl_time)
	if err != nil {
		log.Fatal(err)
	}
	if ttl_time &amp;gt; 0 {
		fmt.Println(11)
		_, err := redis.String(lock.conn.Do(&amp;quot;SET&amp;quot;, lock.key(), lock.token, &amp;quot;EX&amp;quot;, int(ttl_time+ex_time)))
		if err == redis.ErrNil {
			return false, nil
		}
		if err != nil {
			return false, err
		}
	}
	return false, nil
}

func TryLock(conn redis.Conn, resource string, token string, DefaultTimeout int) (lock *Lock, ok bool, err error) {
	return TryLockWithTimeout(conn, resource, token, DefaultTimeout)
}

func TryLockWithTimeout(conn redis.Conn, resource string, token string, timeout int) (lock *Lock, ok bool, err error) {
	lock = &amp;amp;Lock{resource: resource, token: token, conn: conn, timeout: timeout}
	ok, err = lock.tryLock()
	if !ok || err != nil {
		lock = nil
	}
	return
}

func main() {
	fmt.Println(&amp;quot;start&amp;quot;)
	DefaultTimeout := 10
	conn, err := redis.Dial(&amp;quot;tcp&amp;quot;, &amp;quot;localhost:6379&amp;quot;)
	if err != nil {
		log.Fatal(err)
	}
	lock, ok, err := TryLock(conn, &amp;quot;anakin.sun&amp;quot;, &amp;quot;token&amp;quot;, int(DefaultTimeout))
	if err != nil {
		log.Fatal(&amp;quot;error lock&amp;quot;)
	}
	if !ok {
		log.Fatal(&amp;quot;lock fail&amp;quot;)
	}
	lock.AddTimeout(100)
	time.Sleep(time.Duration(DefaultTimeout) * time.Second)
	fmt.Println(&amp;quot;end&amp;quot;)
	defer lock.Unlock()
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      
    </item>
    
    <item>
      <title>TCP协议细节学习</title>
      <link>http://anakin.github.io/post/tcp-detail/</link>
      <pubDate>Tue, 14 May 2019 12:34:43 +0800</pubDate>
      
      <guid>http://anakin.github.io/post/tcp-detail/</guid>
      
        <description>

&lt;h2 id=&#34;tcp协议中包含ip信息么&#34;&gt;TCP协议中包含ip信息么&lt;/h2&gt;

&lt;p&gt;TCP协议中并不包含ip信息，ip信息是在第三层处理的，TCP中处理的是端口信息&lt;/p&gt;

&lt;h2 id=&#34;mss的值是如何计算的&#34;&gt;MSS的值是如何计算的&lt;/h2&gt;

&lt;p&gt;TCP协议中可选的MSS（Maximum Segment Size，最大报文长度））参数，一般使用MTU代替，值为1460。这个值是怎么来的呢？
Maximum Transmission Unit，缩写MTU，中文名是：最大传输单元。
假设MTU值和IP数据包大小一致，一个IP数据包的大小是：65535，那么加上以太网帧头和为，一个以太网帧的大小就是：65535 + 14 + 4 = 65553，看起来似乎很完美，发送方也不需要拆包，接收方也不需要重组。
那么假设我们现在的带宽是：100Mbps，因为以太网帧是传输中的最小可识别单元，再往下就是0101所对应的光信号了，所以我们的一条带宽同时只能发送一个以太网帧。如果同时发送多个，那么对端就无法重组成一个以太网帧了，在100Mbps的带宽中（假设中间没有损耗），我们计算一下发送这一帧需要的时间：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;( 65553 * 8 ) / ( 100 * 1024 * 1024 ) ≈ 0.005(s)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在100M网络下传输一帧就需要5ms，也就是说这5ms其他进程发送不了任何数据。如果是早先的电话拨号，网速只有2M的情况下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;( 65553 * 8 ) / ( 2 * 1024 * 1024 ) ≈ 0.100(s)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;100ms，这简直是噩梦。其实这就像红绿灯，时间要设置合理，交替通行，不然同一个方向如果一直是绿灯，那么另一个方向就要堵成翔了。
既然大了不行，那设置小一点可以么？
假设MTU值设置为100，那么单个帧传输的时间，在2Mbps带宽下需要：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;( 100 * 8 ) / ( 2 * 1024 * 1024 ) * 1000 ≈ 5(ms)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;时间上已经能接受了，问题在于，不管MTU设置为多少，以太网头帧尾大小是固定的，都是14 + 4，所以在MTU为100的时候，一个以太网帧的传输效率为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;( 100 - 14 - 4 ) / 100 = 82%
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;写成公式就是：( T - 14 - 4 ) / T，当T趋于无穷大的时候，效率接近100%，也就是MTU的值越大，传输效率最高，但是基于上一点传输时间的问题，来个折中的选择吧，既然头加尾是18，那就凑个整来个1500，总大小就是1518，传输效率：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1500 / 1518 =  98.8%
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;100Mbps传输时间：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;( 1518 * 8 ) / ( 100 * 1024 * 1024 ) * 1000 = 0.11(ms)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2Mbps传输时间：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;( 1518 * 8 ) / ( 2 * 1024 * 1024 ) * 1000 = 5.79(ms)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;总体上时间都还能接受
因此，1500，是一个折中的结果而已，这就是为啥路由器上一般都设置成这个值。
另外，如果使用PPPoE协议（ADSL）,就需要设置成更小的值，为啥呢，。
PPPoE协议头信息为:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;| VER(4bit) | TYPE(4bit) | CODE(8bit) | SESSION-ID(16bit) | LENGTH(16bit) |
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里总共是48位，也就是6个字节，那么另外2个字节是什么呢？答案是PPP协议的ID号，占用两个字节，所以在PPPoE环境下，最佳MTU值应该是：1500 - 4 - 2 = 1492
说回来，MTU的值的计算，需要从1500中减去IP数据包包头的大小20Bytes和TCP数据段的包头20Bytes，最后就得到了1460。&lt;/p&gt;

&lt;h2 id=&#34;四次挥手的原因&#34;&gt;四次挥手的原因&lt;/h2&gt;

&lt;p&gt;TCP连接是全双工的，即一端接收到FIN报时，对端虽然不再能发送数据，但是可以接收数据，所以需要两边都关闭连接才算完全关闭了这条TCP连接。&lt;/p&gt;

&lt;h2 id=&#34;time-wait状态&#34;&gt;TIME-WAIT状态&lt;/h2&gt;

&lt;p&gt;主动关闭的一方收到对端发出的FIN报之后，就从FIN-WAIT-2状态切换到TIME-WAIT状态了，再等待2MSL时间才再切换到CLOSED状态。这么做的原因在于：&lt;/p&gt;

&lt;p&gt;确保被动关闭的一方有足够的时间收到ACK，如果没有收到会触发重传。
有足够的时间，以让该连接不会与后面的连接混在一起。
TIME-WAIT状态如果过多，会占用系统资源。Linux下有几个参数可以调整TIME-WAIT状态时间：&lt;/p&gt;

&lt;p&gt;net.ipv4.tcp_tw_reuse = 1 表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭。&lt;/p&gt;

&lt;p&gt;net.ipv4.tcp_tw_recycle = 1 表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭。&lt;/p&gt;

&lt;p&gt;net.ipv4.tcp_max_tw_buckets = 5000表示系统同时保持TIME_WAIT套接字的最大数量，如果超过这个数字，TIME_WAIT套接字将立刻被清除并打印警告信息。默认为180000，改为5000。&lt;/p&gt;

&lt;p&gt;然而，从TCP状态转换图可以看出，主动进行关闭的链接才会进入TIME-WAIT状态，所以最好的办法：尽量不要让服务器主动关闭链接，除非一些异常情况，如客户端协议错误、客户端超时等等。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>Golang的GC学习</title>
      <link>http://anakin.github.io/post/golang-gc/</link>
      <pubDate>Sun, 12 May 2019 23:05:48 +0800</pubDate>
      
      <guid>http://anakin.github.io/post/golang-gc/</guid>
      
        <description>

&lt;h2 id=&#34;stw触发的时间&#34;&gt;STW触发的时间&lt;/h2&gt;

&lt;p&gt;一次GC有两次触发STW，一次是GC的开始阶段，主要是开启写屏障和辅助GC等操作
另外就是表记完成之后，重新扫描部分根对象，禁用写屏障&lt;/p&gt;

&lt;h2 id=&#34;gc的触发条件&#34;&gt;GC的触发条件&lt;/h2&gt;

&lt;p&gt;GC在满足一定条件后会被触发, 触发条件有以下几种:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;gcTriggerAlways: 强制触发GC&lt;/li&gt;
&lt;li&gt;gcTriggerHeap: 当前分配的内存达到一定值就触发GC&lt;/li&gt;
&lt;li&gt;gcTriggerTime: 当一定时间没有执行过GC就触发GC&lt;/li&gt;
&lt;li&gt;gcTriggerCycle: 要求启动新一轮的GC, 已启动则跳过, 手动触发GC的runtime.GC()会使用这个条件&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;三色标记的过程&#34;&gt;三色标记的过程&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;所有对象最开始都是白色。&lt;/li&gt;
&lt;li&gt;从 root 开始找到所有可达对象，标记为灰色，放入待处理队列。&lt;/li&gt;
&lt;li&gt;遍历灰色对象队列，将其引用对象标记为灰色放入待处理队列，自身标记为黑色。&lt;/li&gt;
&lt;li&gt;处理完灰色对象队列，执行清扫工作。&lt;/li&gt;
&lt;/ul&gt;
</description>
      
    </item>
    
  </channel>
</rss>
