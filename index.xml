<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Anakin- 奔向NB的生活</title>
    <link>http://anakin.github.io/</link>
    <description>Recent content on Anakin- 奔向NB的生活</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 20 May 2019 15:18:19 +0800</lastBuildDate>
    
        <atom:link href="http://anakin.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>手动搭建kubernetes集群（二）</title>
      <link>http://anakin.github.io/post/create-k8s-2/</link>
      <pubDate>Mon, 20 May 2019 15:18:19 +0800</pubDate>
      
      <guid>http://anakin.github.io/post/create-k8s-2/</guid>
      
        <description>

&lt;p&gt;根据前文准备好的环境，我们现在来一步步的搭建一个基础的k8s集群&lt;br&gt;
&lt;code&gt;注意，这里的配置信息都是按照我自己的虚拟环境来写的。&lt;/code&gt;&lt;br&gt;
把server01作为master节点，server02和server03作为worker节点&lt;br&gt;
各个节点需要配置的服务和命令如下：&lt;br&gt;
master节点上需要部署的服务包括：&lt;code&gt;etcd服务&lt;/code&gt;、&lt;code&gt;APIServer服务&lt;/code&gt;、&lt;code&gt;Scheduler服务&lt;/code&gt;、&lt;code&gt;ControllerManager服务&lt;/code&gt;、&lt;code&gt;CalicoNode服务&lt;/code&gt;、&lt;code&gt;kube-proxy服务&lt;/code&gt;、&lt;code&gt;kubectl命令&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;worker节点上需要部署的服务包括：&lt;code&gt;CalicoNode服务&lt;/code&gt;、&lt;code&gt;kubelet服务&lt;/code&gt;、&lt;code&gt;kube-proxy服务&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&#34;步骤1-准备文件&#34;&gt;步骤1：准备文件&lt;/h2&gt;

&lt;p&gt;安装k8s集群有几种方式可以选择，比如容器化的方式，比如用kubeadmin的方式，这次我们打算尝试的是使用二进制文件的方式。&lt;br&gt;
1. 登录到master虚拟机上（server01），从github上下载安装文件的压缩包，我们使用的是1.13.6版本：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    wget https://github.com/kubernetes/kubernetes/releases/download/v1.13.6/kubernetes.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;解压缩&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tar zxvf kubernetes.tar.gz
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;下载文件，进入刚刚解压好的文件夹&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd kubernetes
./cluster/get-kube-binaries.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个步骤因为涉及到从官网下载文件，由于墙的原因会非常缓慢或者失败，请自行上网解决。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;步骤2-master环境部署&#34;&gt;步骤2：master环境部署&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;etcd部署：&lt;br&gt;
编写etcd服务的启动配置文件etcd.service，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target
Documentation=https://github.com/coreos

[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
ExecStart=/home/anakin/bin/etcd \
--name=192.168.32.131 \
--listen-client-urls=http://192.168.32.131:2379,http://127.0.0.1:2379 \
--advertise-client-urls=http://192.168.32.131:2379 \
--data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;然后执行以下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cp etcd.service /lib/systemd/system/
systemctl enable etcd.service
mkdir -p /var/lib/etcd
service etcd start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果没有问题的话，etcd服务应该已经跑起来了。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;APIServer部署&lt;br&gt;
编写kube-apiserver.service文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target
[Service]
ExecStart=/home/anakin/bin/kube-apiserver \
--admission-control=NamespaceLifecycle,LimitRanger,DefaultStorageClass,ResourceQuota,NodeRestriction \
--insecure-bind-address=0.0.0.0 \
--kubelet-https=false \
--service-cluster-ip-range=10.68.0.0/16 \
--service-node-port-range=20000-40000 \
--etcd-servers=http://192.168.32.131:2379 \
--enable-swagger-ui=true \
--allow-privileged=true \
--audit-log-maxage=30 \
--audit-log-maxbackup=3 \
--audit-log-maxsize=100 \
--audit-log-path=/var/lib/audit.log \
--event-ttl=1h \
--v=2
Restart=on-failure
RestartSec=5
Type=notify
LimitNOFILE=65536
[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后执行和启动etcd类似的命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cp kube-apiserver.service /lib/systemd/system/
systemctl enable kube-apiserver.service
service kube-apiserver start
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ControllerManager部署&lt;br&gt;
编写kube-controller-manager.service文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
[Service]
ExecStart=/home/anakin/bin/kube-controller-manager \
--address=127.0.0.1 \
--master=http://127.0.0.1:8080 \
--allocate-node-cidrs=true \
--service-cluster-ip-range=10.68.0.0/16 \
--cluster-cidr=172.20.0.0/16 \
--cluster-name=kubernetes \
--leader-elect=true \
--cluster-signing-cert-file= \
--cluster-signing-key-file= \
--v=2
Restart=on-failure
RestartSec=5
[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，同样的方式启动服务:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cp kube-controller-manager.service /lib/systemd/system/
systemctl enable kube-controller-manager.service
service kube-controller-manager start
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Scheduler部署&lt;br&gt;
编写kube-scheduler.service文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
ExecStart=/home/anakin/bin/kube-scheduler \
--address=127.0.0.1 \
--master=http://127.0.0.1:8080 \
--leader-elect=true \
--v=2
Restart=on-failure
RestartSec=5
[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;然后，继续上面的启动方式:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cp kube-scheduler.service /lib/systemd/system/
systemctl enable kube-scheduler.service
service kube-scheduler start
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;CalicoNode部署
编写kube-calico.service文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Unit]
Description=calico node
After=docker.service
Requires=docker.service

[Service]
User=root
PermissionsStartOnly=true
ExecStart=/usr/bin/docker run --net=host --privileged --name=calico-node \
-e ETCD_ENDPOINTS=http://192.168.32.131:2379 \
-e CALICO_LIBNETWORK_ENABLED=true \
-e CALICO_NETWORKING_BACKEND=bird \
-e CALICO_DISABLE_FILE_LOGGING=true \
-e CALICO_IPV4POOL_CIDR=172.20.0.0/16 \
-e CALICO_IPV4POOL_IPIP=off \
-e FELIX_DEFAULTENDPOINTTOHOSTACTION=ACCEPT \
-e FELIX_IPV6SUPPORT=false \
-e FELIX_LOGSEVERITYSCREEN=info \
-e FELIX_IPINIPMTU=1440 \
-e FELIX_HEALTHENABLED=true \
-e IP=192.168.32.131 \
-v /var/run/calico:/var/run/calico \
-v /lib/modules:/lib/modules \
-v /run/docker/plugins:/run/docker/plugins \
-v /var/run/docker.sock:/var/run/docker.sock \
-v /var/log/calico:/var/log/calico \
registry.anakin.sun.com/k8s/calico-node:v2.6.2
ExecStop=/usr/bin/docker rm -f calico-node
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;同样的方式启动：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cp kube-calico.service /lib/systemd/system/
systemctl enable kube-calico.service
service kube-calico start
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;kubectl命令配置
执行以下命令(root)：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config set-cluster kubernetes  --server=http://192.168.1.102:8080
kubectl config set-context kubernetes --cluster=kubernetes
kubectl config use-context kubernetes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果有问题，可以手动修改配置文件：~/.kube/config&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;kube-proxy服务部署
编写kube-proxy.service文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target
[Service]
WorkingDirectory=/var/lib/kube-proxy
ExecStart=/home/anakin/bin/kube-proxy \
--bind-address=192.168.32.131 \
--hostname-override=192.168.32.131 \
--kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \
--logtostderr=true \
--v=2
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;编写kube-proxy.kubeconfig文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
clusters:
- cluster:
    server: http://192.168.32.131:8080
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
  name: default
current-context: default
kind: Config
preferences: {}
users: []
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后执行如下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p /var/lib/kube-proxy
cp kube-proxy.service /lib/systemd/system/
cp kube-proxy.kubeconfig /etc/kubernetes/
systemctl enable kube-proxy.service
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;OK,至此，master节点应该已经配置完成了。&lt;/p&gt;

&lt;h2 id=&#34;步骤3-worker环境部署&#34;&gt;步骤3：worker环境部署&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;CalicoNode部署&lt;br&gt;
参考master部分的内容&lt;/li&gt;

&lt;li&gt;&lt;p&gt;kube-proxy服务部署&lt;br&gt;
参考master部分的内容&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;kubelet服务配置&lt;br&gt;
编写kubelet.service文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=docker.service
Requires=docker.service

[Service]
WorkingDirectory=/var/lib/kubelet
ExecStart=/home/anakin/bin/kubelet \
--address=192.168.32.131 \
--hostname-override=192.168.32.131 \
--pod-infra-container-image=registry.anakin.sun.com/imooc/pause-amd64:3.0 \
--kubeconfig=/etc/kubernetes/kubelet.kubeconfig \
--network-plugin=cni \
--cni-conf-dir=/etc/cni/net.d \
--cni-bin-dir=/home/anakin/bin \
--cluster-dns=10.68.0.2 \
--cluster-domain=cluster.local. \
--allow-privileged=true \
--fail-swap-on=false \
--logtostderr=true \
--v=2
ExecStartPost=/sbin/iptables -A INPUT -s 10.0.0.0/8 -p tcp --dport 4194 -j ACCEPT
ExecStartPost=/sbin/iptables -A INPUT -s 172.16.0.0/12 -p tcp --dport 4194 -j ACCEPT
ExecStartPost=/sbin/iptables -A INPUT -s 192.168.0.0/16 -p tcp --dport 4194 -j ACCEPT
ExecStartPost=/sbin/iptables -A INPUT -p tcp --dport 4194 -j DROP
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;编写kubelet.kubeconfig文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: http://192.168.32.131:8080
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: &amp;quot;&amp;quot;
  name: system:node:kube-master
current-context: system:node:kube-master
kind: Config
preferences: {}
users: []
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编写10-calico.conf文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    &amp;quot;name&amp;quot;: &amp;quot;calico-k8s-network&amp;quot;,
    &amp;quot;cniVersion&amp;quot;: &amp;quot;0.1.0&amp;quot;,
    &amp;quot;type&amp;quot;: &amp;quot;calico&amp;quot;,
    &amp;quot;etcd_endpoints&amp;quot;: &amp;quot;http://192.168.32.131:2379&amp;quot;,
    &amp;quot;log_level&amp;quot;: &amp;quot;info&amp;quot;,
    &amp;quot;ipam&amp;quot;: {
        &amp;quot;type&amp;quot;: &amp;quot;calico-ipam&amp;quot;
    },
    &amp;quot;kubernetes&amp;quot;: {
        &amp;quot;k8s_api_root&amp;quot;: &amp;quot;http://192.168.32.131:8080&amp;quot;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，执行如下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p /var/lib/kubelet
mkdir -p /etc/kubernetes
mkdir -p /etc/cni/net.d

cp kubelet.service /lib/systemd/system/
cp kubelet.kubeconfig /etc/kubernetes/
cp 10-calico.conf /etc/cni/net.d/

systemctl enable kubelet.service
service kubelet start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;至此，整个集群应该已经搭建好了，如果中间遇到什么问题，可以通过查看系统日志，或者google解决。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>手动搭建kubernetes集群（一）</title>
      <link>http://anakin.github.io/post/create-k8s/</link>
      <pubDate>Mon, 20 May 2019 10:57:16 +0800</pubDate>
      
      <guid>http://anakin.github.io/post/create-k8s/</guid>
      
        <description>

&lt;p&gt;最近在看有关k8s的一些知识，想手动搭建一套环境来体验一下，本文详细记录一下搭建的过程。&lt;/p&gt;

&lt;h2 id=&#34;环境&#34;&gt;环境&lt;/h2&gt;

&lt;p&gt;三台Ubuntu系统的虚拟机，其中一台作为master，另外两台作为worker节点&lt;/p&gt;

&lt;h2 id=&#34;步骤1-安装虚拟机&#34;&gt;步骤1:安装虚拟机&lt;/h2&gt;

&lt;p&gt;我的笔记本是一台Macbook Pro，首选安装一个虚拟机软件Vmware Fusion，过程略。&lt;br&gt;
1. 下载好Ubuntu镜像，我选择的是&lt;code&gt;19.04&lt;/code&gt;版本。&lt;br&gt;
2. 打开Fusion，选择New，然后选择“Install from disk or image”,continue
3. 设置好用户名和密码，中间还可以修改使用的硬盘空间等等，这个过程就不详述了。
4. 安装好之后，进入系统，找到“terminal”，安装net-tools和ssh server
    &lt;code&gt;
    sudo apt install net-tools ssh
&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&#34;步骤2-安装docker&#34;&gt;步骤2：安装docker&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;更新包列表：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt update
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;让apt支持https方式&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt install apt-transport-https ca-certificates curl software-properties-common
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;添加GPG密钥&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;添加docker源&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo add-apt-repository &amp;quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;再次更新apt&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt update
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;确保从Docker repo安装而不是默认的Ubuntu repo&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apt-cache policy docker-ce
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;开始安装docker&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt install docker-ce
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;接受所有ip的数据包转发&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vi /lib/systemd/system/docker.service
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;找到ExecStart，在这上面加入下面一行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ExecStartPost=/sbin/iptables -I FORWARD -s 0.0.0.0/0 -j ACCEPT
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;重启&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo systemctl daemon-reload
sudo service docker restart
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;步骤3-系统设置&#34;&gt;步骤3：系统设置&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;关闭防火墙&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ufw disable
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;设置系统转发参数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/sysctl.d/k8s.conf
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.conf.all.rp_filter = 0
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;sysctl -p /etc/sysctl.d/k8s.conf
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;安装ntp服务，同步时钟&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt install ntp
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;步骤4-复制虚拟机&#34;&gt;步骤4：复制虚拟机&lt;/h2&gt;

&lt;p&gt;使用fusion的复制功能，复制出另外两台虚拟机&lt;br&gt;
关掉刚装好的虚拟机，选择Fusion菜单Virtual Machine下面的Create Full Clone，Fusion就会clone出一个一模一样的虚拟机出来，是不是很方便&lt;/p&gt;

&lt;h2 id=&#34;步骤5-设置免登陆和hosts文件&#34;&gt;步骤5：设置免登陆和hosts文件&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;打开三台虚拟机，进入terminal，执行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ifconfig
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;找到各自的ip地址&lt;br&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;打开Mac的terminal，输入下面的命令生成ssh公钥&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ssh-keygen -t rsa
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;将公钥分别拷贝到三台虚拟机上&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scp .ssh/id_rsa.pub user@xxx.xxx.xxx.xxx:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;user是虚拟机上的用户名，xxx代表各自的ip地址&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;在虚拟机上将公钥追加到.ssh/authorized_keys文件，并修改权限&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat id_rsa.pub &amp;gt;&amp;gt; .ssh/authorized_keys
chmod 600 authorized_keys
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;分别修改每台虚拟机上的hosts文件，用vim编辑器打开/etc/hosts,添加三台虚拟机的hosts信息，下面是我自己的host信息：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;192.168.32.131 server01
192.168.32.132 server02
192.168.32.133 server03
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;ok，到现在为止，基本的安装环境应该是准备好了。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>Etcd实现MVCC的原理</title>
      <link>http://anakin.github.io/post/etcd-mvcc/</link>
      <pubDate>Sun, 19 May 2019 22:52:01 +0800</pubDate>
      
      <guid>http://anakin.github.io/post/etcd-mvcc/</guid>
      
        <description>

&lt;p&gt;etcd满足的是CAP理论中的CP，实现了最终的强一致，使用Raft协议，Quorum机制（大多数同意原则）,&lt;/p&gt;

&lt;h2 id=&#34;mvcc的意思&#34;&gt;MVCC的意思&lt;/h2&gt;

&lt;p&gt;Multi-Version Concurrency Control 多版本并发控制，目的是为了实现并发访问&lt;/p&gt;

&lt;h2 id=&#34;实现原理&#34;&gt;实现原理&lt;/h2&gt;

&lt;p&gt;在etcd中，MVCC是如何实现的呢，先来看一下相关的代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type revision struct {
main int64
sub int64
} 

type generation struct {
ver     int64
created revision 
revs    []revision
} 

type keyIndex struct {
key         []byte
modified    revision 
generations []generation
} 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;根据代码，可以看出：
* 每个tx事务有唯一事务ID，在etcd中叫做main ID，全局递增不重复。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;一个tx可以包含多个修改操作（put和delete），每一个操作叫做一个revision（修订），共享同一个main ID。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;一个tx内连续的多个修改操作会被从0递增编号，这个编号叫做sub ID。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;每个revision由（main ID，sub ID）唯一标识。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;多个版本的修改历史，保存在generations中&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;每一次操作行为都被单独记录下来，用户value保存到bbolt中。&lt;/p&gt;

&lt;p&gt;在bbolt中，每个revision将作为key，即序列化（revision.main+revision.sub）作为key。因此，我们先通过内存btree在keyIndex.generations[0].revs中找到最后一条revision，即可去bbolt中读取对应的数据。&lt;/p&gt;

&lt;p&gt;相应的，etcd支持按key前缀查询，其实也就是遍历btree的同时根据revision去bbolt中获取用户的value。&lt;/p&gt;

&lt;p&gt;总结一下就是，内存btree维护的是用户key =&amp;gt; keyIndex的映射，keyIndex内维护多版本的revision信息，而revision可以映射到磁盘bbolt中的用户value。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>Redis持久化</title>
      <link>http://anakin.github.io/post/redis-rdb-aof/</link>
      <pubDate>Fri, 17 May 2019 13:22:22 +0800</pubDate>
      
      <guid>http://anakin.github.io/post/redis-rdb-aof/</guid>
      
        <description>

&lt;p&gt;redis的持久化有两种方式，RDB和AOF
 ## RDB:
在指定的时间间隔内，执行指定次数的写操作，则会将内存中的数据写入到磁盘中。即在指定目录下生成一个dump.rdb文件。Redis 重启会通过加载dump.rdb文件恢复数据。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;配置方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;save 900 1
save 300 10
save 60 10000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;意思是， 900秒内有1个更改，300秒内有10个更改以及60秒内有10000个更改，则将内存中的数据快照写入磁盘。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;恢复方法&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;将dump.rdb 文件拷贝到redis的安装目录的bin目录下，重启redis服务即可&lt;/p&gt;

&lt;h2 id=&#34;aof&#34;&gt;AOF&lt;/h2&gt;

&lt;p&gt;采用日志的形式来记录每个写操作，并追加到文件中。Redis 重启的会根据日志文件的内容将写指令从前到后执行一次以完成数据的恢复工作。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;配置方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;appendonly yes
appendfilename &amp;quot;appendonly.aof&amp;quot;
#指定更新条件
# appendfsync always
appendfsync everysec
# appendfsync no
#配置重写触发机制
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;恢复方法&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;将appendonly.aof 文件拷贝到redis的安装目录的bin目录下，重启redis服务即可。如果因为某些原因导致appendonly.aof 文件格式异常，从而导致数据还原失败，可以通过命令redis-check-aof &amp;ndash;fix appendonly.aof 进行修复&lt;/p&gt;

&lt;h2 id=&#34;区别&#34;&gt;区别&lt;/h2&gt;

&lt;p&gt;RDB通过fork的方式进行处理，性能更好
AOF备份所有的操作，数据更完整，但是效率略差，文件相对较大
实际环境中，可以两者同时使用&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>Golang利用redis实现分布式锁</title>
      <link>http://anakin.github.io/post/golang-redis-lock/</link>
      <pubDate>Thu, 16 May 2019 20:11:09 +0800</pubDate>
      
      <guid>http://anakin.github.io/post/golang-redis-lock/</guid>
      
        <description>

&lt;h2 id=&#34;原理&#34;&gt;原理&lt;/h2&gt;

&lt;p&gt;使用SETNX命令(SET if Not eXists)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SETNX key value
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将 key 的值设为 value，当且仅当 key 不存在。若给定的 key 已经存在，则 SETNX 不做任何动作。&lt;/p&gt;

&lt;p&gt;设置成功，返回 1 。&lt;/p&gt;

&lt;p&gt;设置失败，返回 0 。&lt;/p&gt;

&lt;p&gt;为防止获取锁之后，忘记删除，成功后再设置一个过期时间&lt;/p&gt;

&lt;p&gt;以上就是利用redis实现分布式锁的原理&lt;/p&gt;

&lt;h2 id=&#34;代码&#34;&gt;代码&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
	&amp;quot;fmt&amp;quot;
	&amp;quot;github.com/gomodule/redigo/redis&amp;quot;
	&amp;quot;log&amp;quot;
	&amp;quot;time&amp;quot;
)

type Lock struct {
	resource string
	token    string
	conn     redis.Conn
	timeout  int
}

func (lock *Lock) tryLock() (ok bool, err error) {
	_, err = redis.String(lock.conn.Do(&amp;quot;SET&amp;quot;, lock.key(), lock.token, &amp;quot;EX&amp;quot;, int(lock.timeout), &amp;quot;NX&amp;quot;))
	if err == redis.ErrNil {
		return false, nil
	}
	if err != nil {
		return false, err
	}
	return true, nil
}
func (lock *Lock) Unlock() (err error) {
	_, err = lock.conn.Do(&amp;quot;del&amp;quot;, lock.key())
	return
}

func (lock *Lock) key() string {
	return fmt.Sprintf(&amp;quot;redislock:%s&amp;quot;, lock.resource)
}

func (lock *Lock) AddTimeout(ex_time int64) (ok bool, err error) {
	ttl_time, err := redis.Int64(lock.conn.Do(&amp;quot;TTL&amp;quot;, lock.key()))
	fmt.Println(ttl_time)
	if err != nil {
		log.Fatal(err)
	}
	if ttl_time &amp;gt; 0 {
		fmt.Println(11)
		_, err := redis.String(lock.conn.Do(&amp;quot;SET&amp;quot;, lock.key(), lock.token, &amp;quot;EX&amp;quot;, int(ttl_time+ex_time)))
		if err == redis.ErrNil {
			return false, nil
		}
		if err != nil {
			return false, err
		}
	}
	return false, nil
}

func TryLock(conn redis.Conn, resource string, token string, DefaultTimeout int) (lock *Lock, ok bool, err error) {
	return TryLockWithTimeout(conn, resource, token, DefaultTimeout)
}

func TryLockWithTimeout(conn redis.Conn, resource string, token string, timeout int) (lock *Lock, ok bool, err error) {
	lock = &amp;amp;Lock{resource: resource, token: token, conn: conn, timeout: timeout}
	ok, err = lock.tryLock()
	if !ok || err != nil {
		lock = nil
	}
	return
}

func main() {
	fmt.Println(&amp;quot;start&amp;quot;)
	DefaultTimeout := 10
	conn, err := redis.Dial(&amp;quot;tcp&amp;quot;, &amp;quot;localhost:6379&amp;quot;)
	if err != nil {
		log.Fatal(err)
	}
	lock, ok, err := TryLock(conn, &amp;quot;anakin.sun&amp;quot;, &amp;quot;token&amp;quot;, int(DefaultTimeout))
	if err != nil {
		log.Fatal(&amp;quot;error lock&amp;quot;)
	}
	if !ok {
		log.Fatal(&amp;quot;lock fail&amp;quot;)
	}
	lock.AddTimeout(100)
	time.Sleep(time.Duration(DefaultTimeout) * time.Second)
	fmt.Println(&amp;quot;end&amp;quot;)
	defer lock.Unlock()
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      
    </item>
    
    <item>
      <title>TCP协议细节学习</title>
      <link>http://anakin.github.io/post/tcp-detail/</link>
      <pubDate>Tue, 14 May 2019 12:34:43 +0800</pubDate>
      
      <guid>http://anakin.github.io/post/tcp-detail/</guid>
      
        <description>

&lt;h2 id=&#34;tcp协议中包含ip信息么&#34;&gt;TCP协议中包含ip信息么&lt;/h2&gt;

&lt;p&gt;TCP协议中并不包含ip信息，ip信息是在第三层处理的，TCP中处理的是端口信息&lt;/p&gt;

&lt;h2 id=&#34;mss的值是如何计算的&#34;&gt;MSS的值是如何计算的&lt;/h2&gt;

&lt;p&gt;TCP协议中可选的MSS（Maximum Segment Size，最大报文长度））参数，一般使用MTU代替，值为1460。这个值是怎么来的呢？
Maximum Transmission Unit，缩写MTU，中文名是：最大传输单元。
假设MTU值和IP数据包大小一致，一个IP数据包的大小是：65535，那么加上以太网帧头和为，一个以太网帧的大小就是：65535 + 14 + 4 = 65553，看起来似乎很完美，发送方也不需要拆包，接收方也不需要重组。
那么假设我们现在的带宽是：100Mbps，因为以太网帧是传输中的最小可识别单元，再往下就是0101所对应的光信号了，所以我们的一条带宽同时只能发送一个以太网帧。如果同时发送多个，那么对端就无法重组成一个以太网帧了，在100Mbps的带宽中（假设中间没有损耗），我们计算一下发送这一帧需要的时间：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;( 65553 * 8 ) / ( 100 * 1024 * 1024 ) ≈ 0.005(s)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在100M网络下传输一帧就需要5ms，也就是说这5ms其他进程发送不了任何数据。如果是早先的电话拨号，网速只有2M的情况下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;( 65553 * 8 ) / ( 2 * 1024 * 1024 ) ≈ 0.100(s)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;100ms，这简直是噩梦。其实这就像红绿灯，时间要设置合理，交替通行，不然同一个方向如果一直是绿灯，那么另一个方向就要堵成翔了。
既然大了不行，那设置小一点可以么？
假设MTU值设置为100，那么单个帧传输的时间，在2Mbps带宽下需要：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;( 100 * 8 ) / ( 2 * 1024 * 1024 ) * 1000 ≈ 5(ms)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;时间上已经能接受了，问题在于，不管MTU设置为多少，以太网头帧尾大小是固定的，都是14 + 4，所以在MTU为100的时候，一个以太网帧的传输效率为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;( 100 - 14 - 4 ) / 100 = 82%
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;写成公式就是：( T - 14 - 4 ) / T，当T趋于无穷大的时候，效率接近100%，也就是MTU的值越大，传输效率最高，但是基于上一点传输时间的问题，来个折中的选择吧，既然头加尾是18，那就凑个整来个1500，总大小就是1518，传输效率：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1500 / 1518 =  98.8%
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;100Mbps传输时间：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;( 1518 * 8 ) / ( 100 * 1024 * 1024 ) * 1000 = 0.11(ms)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2Mbps传输时间：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;( 1518 * 8 ) / ( 2 * 1024 * 1024 ) * 1000 = 5.79(ms)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;总体上时间都还能接受
因此，1500，是一个折中的结果而已，这就是为啥路由器上一般都设置成这个值。
另外，如果使用PPPoE协议（ADSL）,就需要设置成更小的值，为啥呢，。
PPPoE协议头信息为:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;| VER(4bit) | TYPE(4bit) | CODE(8bit) | SESSION-ID(16bit) | LENGTH(16bit) |
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里总共是48位，也就是6个字节，那么另外2个字节是什么呢？答案是PPP协议的ID号，占用两个字节，所以在PPPoE环境下，最佳MTU值应该是：1500 - 4 - 2 = 1492
说回来，MTU的值的计算，需要从1500中减去IP数据包包头的大小20Bytes和TCP数据段的包头20Bytes，最后就得到了1460。&lt;/p&gt;

&lt;h2 id=&#34;四次挥手的原因&#34;&gt;四次挥手的原因&lt;/h2&gt;

&lt;p&gt;TCP连接是全双工的，即一端接收到FIN报时，对端虽然不再能发送数据，但是可以接收数据，所以需要两边都关闭连接才算完全关闭了这条TCP连接。&lt;/p&gt;

&lt;h2 id=&#34;time-wait状态&#34;&gt;TIME-WAIT状态&lt;/h2&gt;

&lt;p&gt;主动关闭的一方收到对端发出的FIN报之后，就从FIN-WAIT-2状态切换到TIME-WAIT状态了，再等待2MSL时间才再切换到CLOSED状态。这么做的原因在于：&lt;/p&gt;

&lt;p&gt;确保被动关闭的一方有足够的时间收到ACK，如果没有收到会触发重传。
有足够的时间，以让该连接不会与后面的连接混在一起。
TIME-WAIT状态如果过多，会占用系统资源。Linux下有几个参数可以调整TIME-WAIT状态时间：&lt;/p&gt;

&lt;p&gt;net.ipv4.tcp_tw_reuse = 1 表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭。&lt;/p&gt;

&lt;p&gt;net.ipv4.tcp_tw_recycle = 1 表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭。&lt;/p&gt;

&lt;p&gt;net.ipv4.tcp_max_tw_buckets = 5000表示系统同时保持TIME_WAIT套接字的最大数量，如果超过这个数字，TIME_WAIT套接字将立刻被清除并打印警告信息。默认为180000，改为5000。&lt;/p&gt;

&lt;p&gt;然而，从TCP状态转换图可以看出，主动进行关闭的链接才会进入TIME-WAIT状态，所以最好的办法：尽量不要让服务器主动关闭链接，除非一些异常情况，如客户端协议错误、客户端超时等等。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>Golang的GC学习</title>
      <link>http://anakin.github.io/post/golang-gc/</link>
      <pubDate>Sun, 12 May 2019 23:05:48 +0800</pubDate>
      
      <guid>http://anakin.github.io/post/golang-gc/</guid>
      
        <description>

&lt;h2 id=&#34;stw触发的时间&#34;&gt;STW触发的时间&lt;/h2&gt;

&lt;p&gt;一次GC有两次触发STW，一次是GC的开始阶段，主要是开启写屏障和辅助GC等操作
另外就是表记完成之后，重新扫描部分根对象，禁用写屏障&lt;/p&gt;

&lt;h2 id=&#34;gc的触发条件&#34;&gt;GC的触发条件&lt;/h2&gt;

&lt;p&gt;GC在满足一定条件后会被触发, 触发条件有以下几种:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;gcTriggerAlways: 强制触发GC&lt;/li&gt;
&lt;li&gt;gcTriggerHeap: 当前分配的内存达到一定值就触发GC&lt;/li&gt;
&lt;li&gt;gcTriggerTime: 当一定时间没有执行过GC就触发GC&lt;/li&gt;
&lt;li&gt;gcTriggerCycle: 要求启动新一轮的GC, 已启动则跳过, 手动触发GC的runtime.GC()会使用这个条件&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;三色标记的过程&#34;&gt;三色标记的过程&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;所有对象最开始都是白色。&lt;/li&gt;
&lt;li&gt;从 root 开始找到所有可达对象，标记为灰色，放入待处理队列。&lt;/li&gt;
&lt;li&gt;遍历灰色对象队列，将其引用对象标记为灰色放入待处理队列，自身标记为黑色。&lt;/li&gt;
&lt;li&gt;处理完灰色对象队列，执行清扫工作。&lt;/li&gt;
&lt;/ul&gt;
</description>
      
    </item>
    
  </channel>
</rss>
