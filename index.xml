<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Anakin- 奔向NB的生活</title>
    <link>http://anakin.github.io/</link>
    <description>Recent content on Anakin- 奔向NB的生活</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Fri, 31 May 2019 12:58:45 +0800</lastBuildDate>
    
        <atom:link href="http://anakin.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Laravel整合gRPC</title>
      <link>http://anakin.github.io/post/php-laravel-grpc/</link>
      <pubDate>Fri, 31 May 2019 12:58:45 +0800</pubDate>
      
      <guid>http://anakin.github.io/post/php-laravel-grpc/</guid>
      
        <description>

&lt;p&gt;最近需要用Laravel实现一个Rest的API，后端调用Golang的RPC服务，记录一下整合的过程。&lt;/p&gt;

&lt;h2 id=&#34;环境准备&#34;&gt;环境准备&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;php安装grpc和protobuf扩展&lt;br&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pecl install grpc
pecl install protobuf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;找到php.ini文件，添加下面两行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;extension=grpc.so
extension=protobuf.so
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;clone一个grpc的repo&lt;br&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;git clone https://github.com/grpc/grpc.git
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;假设目录在/home/aaa/下面&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;安装protobuf&lt;br&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apt install protobuf
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;准备proto文件&#34;&gt;准备proto文件&lt;/h2&gt;

&lt;p&gt;文件名：userrpc.proto，内容如下&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;syntax = &amp;quot;proto3&amp;quot;;
package App.UserRpc;


service User{
  rpc UserLogin(LoginInfo) returns (UserInfo) {}
}

message LoginInfo{
  string loginname= 1;
  string password= 2;
}


message UserInfo{
 int32 code= 1;
  string err_msg= 2;
  string token= 3;
  int32 userid= 4;
  string username= 5;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中定义了两种消息结构和一个rpc服务。&lt;/p&gt;

&lt;h2 id=&#34;生成php的grpc客户端代码&#34;&gt;生成PHP的gRPC客户端代码&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;在user.proto文件的所在目录，执行下面的命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;protoc --php_out=./ --grpc_out=./ --plugin=protoc-gen-grpc=/home/aaa/grpc/bins/opt/grpc_php_plugin user.proto
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果没问题的话，应该会生成两个目录:App和GPBMetadata&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;拷贝文件&lt;br&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;把App下面的UserRpc目录拷贝到laravel的app/下，把GPBMetadata目录拷贝到和app同级的目录。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;设置composer&lt;br&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;打开laravel的composer.json文件，在classmap下添加一行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;GPBMetadata
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行下面的命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;composer require &amp;quot;grpc/grpc&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后执行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;composer dump-autoload
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;编写controller&#34;&gt;编写Controller&lt;/h2&gt;

&lt;p&gt;现在可以去写一个laravel的controller试试了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  public function login(Request $request){
    $loginname = $request-&amp;gt;input(&amp;quot;loginname&amp;quot;);
    $password = (string)$request-&amp;gt;input(&amp;quot;password&amp;quot;);
    $userrpc = new \App\UserRpc\UserClient(&amp;quot;127.0.0.1:50052&amp;quot;,[
                                &#39;credentials&#39; =&amp;gt; \Grpc\ChannelCredentials::createInsecure()
                ]);
    $request = new \App\UserRpc\LoginInfo();
    $request-&amp;gt;setLoginname($loginname);
    $request-&amp;gt;setPassword($password);
    list($recv,$status) = $userrpc-&amp;gt;UserLogin($request)-&amp;gt;wait();
    $code = $recv-&amp;gt;getCode();
    echo $code;
    exit;
  }
&lt;/code&gt;&lt;/pre&gt;
</description>
      
    </item>
    
    <item>
      <title>Gokit创建微服务的例子(翻译)</title>
      <link>http://anakin.github.io/post/gokit-example/</link>
      <pubDate>Mon, 27 May 2019 23:14:16 +0800</pubDate>
      
      <guid>http://anakin.github.io/post/gokit-example/</guid>
      
        <description>

&lt;h2 id=&#34;第一原理&#34;&gt;第一原理&lt;/h2&gt;

&lt;p&gt;让我们创建一个Go kit 服务，我们暂时只用一个main.go文件。&lt;br&gt;&lt;/p&gt;

&lt;h3 id=&#34;你的事务逻辑-br&#34;&gt;你的事务逻辑&lt;br&gt;&lt;/h3&gt;

&lt;p&gt;你的服务从你的事务逻辑开始。在Go kit里，我们用一个interface来描述一个服务&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// StringService provides operations on strings.
import &amp;quot;context&amp;quot;

type StringService interface {
	Uppercase(string) (string, error)
	Count(string) int
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个interface会有一个实现&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import (
	&amp;quot;context&amp;quot;
	&amp;quot;errors&amp;quot;
	&amp;quot;strings&amp;quot;
)

type stringService struct{}

func (stringService) Uppercase(s string) (string, error) {
	if s == &amp;quot;&amp;quot; {
		return &amp;quot;&amp;quot;, ErrEmpty
	}
	return strings.ToUpper(s), nil
}

func (stringService) Count(s string) int {
	return len(s)
}

// ErrEmpty is returned when input string is empty
var ErrEmpty = errors.New(&amp;quot;Empty string&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;请求和应答&#34;&gt;请求和应答&lt;/h3&gt;

&lt;p&gt;在Go kit里，主要的通信模式是RPC。因此，我们的interface里的每个方法都会被作为一个远程过程调用。我们为每个方法定义请求和应答结构，独立的捕获所有的输入和输出。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type uppercaseRequest struct {
	S string `json:&amp;quot;s&amp;quot;`
}

type uppercaseResponse struct {
	V   string `json:&amp;quot;v&amp;quot;`
	Err string `json:&amp;quot;err,omitempty&amp;quot;` // errors don&#39;t JSON-marshal, so we use a string
}

type countRequest struct {
	S string `json:&amp;quot;s&amp;quot;`
}

type countResponse struct {
	V int `json:&amp;quot;v&amp;quot;`
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;终端
Go kit通过一个叫做endpoint的抽象来提供大部分功能。一个endpint的定义如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Endpoint func(ctx context.Context, request interface{}) (response interface{}, err error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;它相当于一个独立的RPC，就是说，我们服务接口里一个独立的方法。我们会写简单的适配器来把我们服务里的每个方法转换成一个endpoint。每个适配器携带一个StringService，返回相当于一个方法的endpoint。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import (
	&amp;quot;context&amp;quot;
	&amp;quot;github.com/go-kit/kit/endpoint&amp;quot;
)

func makeUppercaseEndpoint(svc StringService) endpoint.Endpoint {
	return func(_ context.Context, request interface{}) (interface{}, error) {
		req := request.(uppercaseRequest)
		v, err := svc.Uppercase(req.S)
		if err != nil {
			return uppercaseResponse{v, err.Error()}, nil
		}
		return uppercaseResponse{v, &amp;quot;&amp;quot;}, nil
	}
}

func makeCountEndpoint(svc StringService) endpoint.Endpoint {
	return func(_ context.Context, request interface{}) (interface{}, error) {
		req := request.(countRequest)
		v := svc.Count(req.S)
		return countResponse{v}, nil
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;传输&#34;&gt;传输&lt;/h3&gt;

&lt;p&gt;现在我们需要把你的服务暴露给外面的世界，这样它才能被调用。你的组织对于服务直接相互调用可能已经有了一些选择。可能你用Thrit，或者定制化的通过HTTP传输的JSON。Go kit多种向外的传输方式。&lt;br&gt;
这个小服务，我们用通过HTTP传输的JSON。Go kit在transport/http包里提供了一个helper struct。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import (
	&amp;quot;context&amp;quot;
	&amp;quot;encoding/json&amp;quot;
	&amp;quot;log&amp;quot;
	&amp;quot;net/http&amp;quot;

	httptransport &amp;quot;github.com/go-kit/kit/transport/http&amp;quot;
)

func main() {
	svc := stringService{}

	uppercaseHandler := httptransport.NewServer(
		makeUppercaseEndpoint(svc),
		decodeUppercaseRequest,
		encodeResponse,
	)

	countHandler := httptransport.NewServer(
		makeCountEndpoint(svc),
		decodeCountRequest,
		encodeResponse,
	)

	http.Handle(&amp;quot;/uppercase&amp;quot;, uppercaseHandler)
	http.Handle(&amp;quot;/count&amp;quot;, countHandler)
	log.Fatal(http.ListenAndServe(&amp;quot;:8080&amp;quot;, nil))
}

func decodeUppercaseRequest(_ context.Context, r *http.Request) (interface{}, error) {
	var request uppercaseRequest
	if err := json.NewDecoder(r.Body).Decode(&amp;amp;request); err != nil {
		return nil, err
	}
	return request, nil
}

func decodeCountRequest(_ context.Context, r *http.Request) (interface{}, error) {
	var request countRequest
	if err := json.NewDecoder(r.Body).Decode(&amp;amp;request); err != nil {
		return nil, err
	}
	return request, nil
}

func encodeResponse(_ context.Context, w http.ResponseWriter, response interface{}) error {
	return json.NewEncoder(w).Encode(response)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;stringsvc1&#34;&gt;stringsvc1&lt;/h3&gt;

&lt;p&gt;到现在为止，整个服务就是&lt;a href=&#34;https://github.com/go-kit/kit/blob/master/examples/stringsvc1&#34;&gt;stringsvc1&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go get github.com/go-kit/kit/examples/stringsvc1
$ stringsvc1
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;$ curl -XPOST -d&#39;{&amp;quot;s&amp;quot;:&amp;quot;hello, world&amp;quot;}&#39; localhost:8080/uppercase
{&amp;quot;v&amp;quot;:&amp;quot;HELLO, WORLD&amp;quot;,&amp;quot;err&amp;quot;:null}
$ curl -XPOST -d&#39;{&amp;quot;s&amp;quot;:&amp;quot;hello, world&amp;quot;}&#39; localhost:8080/count
{&amp;quot;v&amp;quot;:12}
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;中间件&#34;&gt;中间件&lt;/h2&gt;

&lt;p&gt;缺少了logging和instrumentation的服务，不能说做好了在生产环境使用的准备。&lt;/p&gt;

&lt;h3 id=&#34;焦虑的分离&#34;&gt;焦虑的分离&lt;/h3&gt;

&lt;p&gt;当你增加服务的endpoint数量的时候，将调用图的每一层分离为单独的文件是一个go-kit工程变得更可读。我们的第一个例子&lt;a href=&#34;https://github.com/go-kit/kit/blob/master/examples/stringsvc1&#34;&gt;stringsvc1&lt;/a&gt;在一个main文件里包含了所有这些层。在增加更多复杂性之前，让我们把代码分离到下列文件中，剩下的留在main.go里。&lt;/p&gt;

&lt;p&gt;把服务放在service.go文件里，包含下面的function和type。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type StringService
type stringService
var ErrEmpty
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;把transports放在transport.go里，包含如下的function和type。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func makeUppercaseEndpoint
func makeCountEndpoint
func decodeUppercaseRequest
func decodeCountRequest
func encodeResponse
type uppercaseRequest
type uppercaseResponse
type countRequest
type countResponse
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;传输日志&#34;&gt;传输日志&lt;/h3&gt;

&lt;p&gt;任何需要记录日志的组件都应该把logger当做一个依赖，就行对待一个数据库连接一样。因此，我们在func main里构建我们的logger，然后传给需要它的组件。我们从不使用一个全局的logger。&lt;/p&gt;

&lt;p&gt;我们本可以直接把logger传递给stringService的实现，但是有更好的办法。让我们用一个中间件。也叫做装饰器。一个中间件就是一个传入endpoint然后返回endpoint的函数。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Middleware func(Endpoint) Endpoint
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意，Middleware类型是go-kit提供给你的。&lt;/p&gt;

&lt;p&gt;在这中间，它可以做任何事情。下面你可以看到一个可以被实现的基本的日志中间件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; loggingMiddleware(logger log.Logger) Middleware {
	return func(next endpoint.Endpoint) endpoint.Endpoint {
		return func(_ context.Context, request interface{}) (interface{}, error) {
			logger.Log(&amp;quot;msg&amp;quot;, &amp;quot;calling endpoint&amp;quot;)
			defer logger.Log(&amp;quot;msg&amp;quot;, &amp;quot;called endpoint&amp;quot;)
			return next(request)
		}
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用go-kit log包，并且删除标准库log。你需要从main.go底部删除log.Fatal。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import (
 &amp;quot;github.com/go-kit/kit/log&amp;quot;
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;把它写在每个处理器里。注意，下面的代码块在你学完Application Logging部分之前不会编译，它定义了loggingMiddleware。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;logger := log.NewLogfmtLogger(os.Stderr)

svc := stringService{}

var uppercase endpoint.Endpoint
uppercase = makeUppercaseEndpoint(svc)
uppercase = loggingMiddleware(log.With(logger, &amp;quot;method&amp;quot;, &amp;quot;uppercase&amp;quot;))(uppercase)

var count endpoint.Endpoint
count = makeCountEndpoint(svc)
count = loggingMiddleware(log.With(logger, &amp;quot;method&amp;quot;, &amp;quot;count&amp;quot;))(count)

uppercaseHandler := httptransport.NewServer(
	// ...
	uppercase,
	// ...
)

countHandler := httptransport.NewServer(
	// ...
	count,
	// ...
)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;它说明这个技术有更多的用途，而不只是用来记录日志。很多Go kit组件都实现成了endpoint的中间件。&lt;/p&gt;

&lt;h3 id=&#34;应用日志&#34;&gt;应用日志&lt;/h3&gt;

&lt;p&gt;但是，如果我们想像传入的参数一样记录我们应用范畴的日志呢？我们可以为我们的服务定义一个中间件，获得一样优雅的组合效果。由于我们的StringService被定义成立一个interface，我们只需要一个包装了已有的StringService的新类型，来负责额外的日志记录。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type loggingMiddleware struct {
	logger log.Logger
	next   StringService
}

func (mw loggingMiddleware) Uppercase(s string) (output string, err error) {
	defer func(begin time.Time) {
		mw.logger.Log(
			&amp;quot;method&amp;quot;, &amp;quot;uppercase&amp;quot;,
			&amp;quot;input&amp;quot;, s,
			&amp;quot;output&amp;quot;, output,
			&amp;quot;err&amp;quot;, err,
			&amp;quot;took&amp;quot;, time.Since(begin),
		)
	}(time.Now())

	output, err = mw.next.Uppercase(s)
	return
}

func (mw loggingMiddleware) Count(s string) (n int) {
	defer func(begin time.Time) {
		mw.logger.Log(
			&amp;quot;method&amp;quot;, &amp;quot;count&amp;quot;,
			&amp;quot;input&amp;quot;, s,
			&amp;quot;n&amp;quot;, n,
			&amp;quot;took&amp;quot;, time.Since(begin),
		)
	}(time.Now())

	n = mw.next.Count(s)
	return
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，写入&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import (
	&amp;quot;os&amp;quot;

	&amp;quot;github.com/go-kit/kit/log&amp;quot;
	httptransport &amp;quot;github.com/go-kit/kit/transport/http&amp;quot;
)

func main() {
	logger := log.NewLogfmtLogger(os.Stderr)

	var svc StringService
	svc = stringService{}
	svc = loggingMiddleware{logger, svc}

	// ...

	uppercaseHandler := httptransport.NewServer(
		// ...
		makeUppercaseEndpoint(svc),
		// ...
	)

	countHandler := httptransport.NewServer(
		// ...
		makeCountEndpoint(svc),
		// ...
	)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;用endpoint中间件对付传输范畴的问题，例如熔断和限流。用服务中间件来处理事务领域的问题，例如日志和instrumentation。说道instrumentation。。。&lt;/p&gt;

&lt;h3 id=&#34;应用instrumentation&#34;&gt;应用instrumentation&lt;/h3&gt;

&lt;p&gt;在Go kit里，instrumentation表示用package metrics来记录关于你的服务的运行时行为的统计信息。统计job运行的数量，在请求结束后记录时长，跟踪正在运行的操作数量，这些都被认为是instrumentation。&lt;/p&gt;

&lt;p&gt;我们可以用和logging相同的中间件模式&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type instrumentingMiddleware struct {
	requestCount   metrics.Counter
	requestLatency metrics.Histogram
	countResult    metrics.Histogram
	next           StringService
}

func (mw instrumentingMiddleware) Uppercase(s string) (output string, err error) {
	defer func(begin time.Time) {
		lvs := []string{&amp;quot;method&amp;quot;, &amp;quot;uppercase&amp;quot;, &amp;quot;error&amp;quot;, fmt.Sprint(err != nil)}
		mw.requestCount.With(lvs...).Add(1)
		mw.requestLatency.With(lvs...).Observe(time.Since(begin).Seconds())
	}(time.Now())

	output, err = mw.next.Uppercase(s)
	return
}

func (mw instrumentingMiddleware) Count(s string) (n int) {
	defer func(begin time.Time) {
		lvs := []string{&amp;quot;method&amp;quot;, &amp;quot;count&amp;quot;, &amp;quot;error&amp;quot;, &amp;quot;false&amp;quot;}
		mw.requestCount.With(lvs...).Add(1)
		mw.requestLatency.With(lvs...).Observe(time.Since(begin).Seconds())
		mw.countResult.Observe(float64(n))
	}(time.Now())

	n = mw.next.Count(s)
	return
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后写入我们的服务里&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import (
	stdprometheus &amp;quot;github.com/prometheus/client_golang/prometheus&amp;quot;
	kitprometheus &amp;quot;github.com/go-kit/kit/metrics/prometheus&amp;quot;
	&amp;quot;github.com/go-kit/kit/metrics&amp;quot;
)

func main() {
	logger := log.NewLogfmtLogger(os.Stderr)

	fieldKeys := []string{&amp;quot;method&amp;quot;, &amp;quot;error&amp;quot;}
	requestCount := kitprometheus.NewCounterFrom(stdprometheus.CounterOpts{
		Namespace: &amp;quot;my_group&amp;quot;,
		Subsystem: &amp;quot;string_service&amp;quot;,
		Name:      &amp;quot;request_count&amp;quot;,
		Help:      &amp;quot;Number of requests received.&amp;quot;,
	}, fieldKeys)
	requestLatency := kitprometheus.NewSummaryFrom(stdprometheus.SummaryOpts{
		Namespace: &amp;quot;my_group&amp;quot;,
		Subsystem: &amp;quot;string_service&amp;quot;,
		Name:      &amp;quot;request_latency_microseconds&amp;quot;,
		Help:      &amp;quot;Total duration of requests in microseconds.&amp;quot;,
	}, fieldKeys)
	countResult := kitprometheus.NewSummaryFrom(stdprometheus.SummaryOpts{
		Namespace: &amp;quot;my_group&amp;quot;,
		Subsystem: &amp;quot;string_service&amp;quot;,
		Name:      &amp;quot;count_result&amp;quot;,
		Help:      &amp;quot;The result of each count method.&amp;quot;,
	}, []string{}) // no fields here

	var svc StringService
	svc = stringService{}
	svc = loggingMiddleware{logger, svc}
	svc = instrumentingMiddleware{requestCount, requestLatency, countResult, svc}

	uppercaseHandler := httptransport.NewServer(
		makeUppercaseEndpoint(svc),
		decodeUppercaseRequest,
		encodeResponse,
	)

	countHandler := httptransport.NewServer(
		makeCountEndpoint(svc),
		decodeCountRequest,
		encodeResponse,
	)

	http.Handle(&amp;quot;/uppercase&amp;quot;, uppercaseHandler)
	http.Handle(&amp;quot;/count&amp;quot;, countHandler)
	http.Handle(&amp;quot;/metrics&amp;quot;, promhttp.Handler())
	logger.Log(&amp;quot;msg&amp;quot;, &amp;quot;HTTP&amp;quot;, &amp;quot;addr&amp;quot;, &amp;quot;:8080&amp;quot;)
	logger.Log(&amp;quot;err&amp;quot;, http.ListenAndServe(&amp;quot;:8080&amp;quot;, nil))
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;stringsvc2&#34;&gt;stringsvc2&lt;/h3&gt;

&lt;p&gt;到现在为止，完整的服务是&lt;a href=&#34;https://github.com/go-kit/kit/blob/master/examples/stringsvc2&#34;&gt;stringsvc2&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go get github.com/go-kit/kit/examples/stringsvc2
$ stringsvc2
msg=HTTP addr=:8080
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;$ curl -XPOST -d&#39;{&amp;quot;s&amp;quot;:&amp;quot;hello, world&amp;quot;}&#39; localhost:8080/uppercase
{&amp;quot;v&amp;quot;:&amp;quot;HELLO, WORLD&amp;quot;,&amp;quot;err&amp;quot;:null}
$ curl -XPOST -d&#39;{&amp;quot;s&amp;quot;:&amp;quot;hello, world&amp;quot;}&#39; localhost:8080/count
{&amp;quot;v&amp;quot;:12}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;method=uppercase input=&amp;quot;hello, world&amp;quot; output=&amp;quot;HELLO, WORLD&amp;quot; err=null took=2.455µs
method=count input=&amp;quot;hello, world&amp;quot; n=12 took=743ns
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;调用其他服务&#34;&gt;调用其他服务&lt;/h2&gt;

&lt;p&gt;一个服务存在于一个真空里是很罕见的。通常，你需要调用其他的服务。&lt;strong&gt;这就是Go kit的价值所在&lt;/strong&gt;。我们提供了传输中间件来解决接下来的很多问题。&lt;/p&gt;

&lt;p&gt;假设我们让我们的string服务调用一个不同的string服务来满足Uppercase方法。就是说，把请求代理到另外的服务上。让我们来实现代理中间件叫做ServiceMiddleware，和logging或者instrumenting中间件一样。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;// proxymw implements StringService, forwarding Uppercase requests to the
// provided endpoint, and serving all other (i.e. Count) requests via the
// next StringService.
type proxymw struct {
	next      StringService     // Serve most requests via this service...
	uppercase endpoint.Endpoint // ...except Uppercase, which gets served by this endpoint
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;客户端-endpoint&#34;&gt;客户端 endpoint&lt;/h3&gt;

&lt;p&gt;我们有一个我们已经了解的同样的endpont，但是我们将用它来转移一个请求，而不是直接提供服务。这样使用的时候，我们叫它客户端endpoint。我们只需要做一些简单的改动：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func (mw proxymw) Uppercase(s string) (string, error) {
	response, err := mw.uppercase(uppercaseRequest{S: s})
	if err != nil {
		return &amp;quot;&amp;quot;, err
	}
	resp := response.(uppercaseResponse)
	if resp.Err != &amp;quot;&amp;quot; {
		return resp.V, errors.New(resp.Err)
	}
	return resp.V, nil
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在，为了构建一个这样的代理中间件，我们转换一个代理URL字符串给一个endpoint。如果我们假设使用通过HTTP的JSON，我们可以使用transport/http包里的helper。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import (
	httptransport &amp;quot;github.com/go-kit/kit/transport/http&amp;quot;
)

func proxyingMiddleware(proxyURL string) ServiceMiddleware {
	return func(next StringService) StringService {
		return proxymw{next, makeUppercaseProxy(proxyURL)}
	}
}

func makeUppercaseProxy(proxyURL string) endpoint.Endpoint {
	return httptransport.NewClient(
		&amp;quot;GET&amp;quot;,
		mustParseURL(proxyURL),
		encodeUppercaseRequest,
		decodeUppercaseResponse,
	).Endpoint()
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;服务发现和负载均衡&#34;&gt;服务发现和负载均衡&lt;/h3&gt;

&lt;p&gt;如果我们只有一个远程服务还好。但是真实情况是，我们可能有很多可用的服务实例。我们希望通过一些服务发现机制发现它们。并且把我们的压力分散到他们身上。而且如果这些实例中的任何一个表现异常，我们希望可以处理，不影响我们的服务可用性。&lt;/p&gt;

&lt;p&gt;Go kit为不同的服务发现系统提供了转换器，用来获取更新的实例集合，作为独立的endpoint暴露出去。这些转换器叫做subscriber。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Subscriber interface {
	Endpoints() ([]endpoint.Endpoint, error)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在内部，subscriber使用一个工厂方法来把发现实例字符串转换成一个可用的endpoint。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Factory func(instance string) (endpoint.Endpoint, error)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;目前，我们的工厂方法，makeUppercaseProxy，还是直接调用URL。但是在工厂里放一些安全中间件，例如熔断和限流，同样很重要。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;var e endpoint.Endpoint
e = makeUppercaseProxy(instance)
e = circuitbreaker.Gobreaker(gobreaker.NewCircuitBreaker(gobreaker.Settings{}))(e)
e = kitratelimit.NewTokenBucketLimiter(jujuratelimit.NewBucketWithRate(float64(maxQPS), int64(maxQPS)))(e)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在，我们有了一个endpoint集合，我们需要选择一个。负载均衡器包装了subscriber，并且从中选择一个endpoint。Go kit提供了一些基础的负载均衡器，并且，如果有更多需求，自己写一个也很容易。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type Balancer interface {
	Endpoint() (endpoint.Endpoint, error)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;现在，我们有能力根据一些逻辑选择endpoint。我们可以用来提供独立的，逻辑的，健壮的endpoint给消费者。重试策略包装一个负载均衡器，返回一个可用的endpoint。重试策略将会重试失败的请求，知道到达最大尝试次数或者超时。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func Retry(max int, timeout time.Duration, lb Balancer) endpoint.Endpoint
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;让我们把最终的代理中间件连起来。简化起见，我们假设用户提供多个用逗号分隔的实例endpoint。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;func proxyingMiddleware(instances string, logger log.Logger) ServiceMiddleware {
	// If instances is empty, don&#39;t proxy.
	if instances == &amp;quot;&amp;quot; {
		logger.Log(&amp;quot;proxy_to&amp;quot;, &amp;quot;none&amp;quot;)
		return func(next StringService) StringService { return next }
	}

	// Set some parameters for our client.
	var (
		qps         = 100                    // beyond which we will return an error
		maxAttempts = 3                      // per request, before giving up
		maxTime     = 250 * time.Millisecond // wallclock time, before giving up
	)

	// Otherwise, construct an endpoint for each instance in the list, and add
	// it to a fixed set of endpoints. In a real service, rather than doing this
	// by hand, you&#39;d probably use package sd&#39;s support for your service
	// discovery system.
	var (
		instanceList = split(instances)
		subscriber   sd.FixedSubscriber
	)
	logger.Log(&amp;quot;proxy_to&amp;quot;, fmt.Sprint(instanceList))
	for _, instance := range instanceList {
		var e endpoint.Endpoint
		e = makeUppercaseProxy(instance)
		e = circuitbreaker.Gobreaker(gobreaker.NewCircuitBreaker(gobreaker.Settings{}))(e)
		e = kitratelimit.NewTokenBucketLimiter(jujuratelimit.NewBucketWithRate(float64(qps), int64(qps)))(e)
		subscriber = append(subscriber, e)
	}

	// Now, build a single, retrying, load-balancing endpoint out of all of
	// those individual endpoints.
	balancer := lb.NewRoundRobin(subscriber)
	retry := lb.Retry(maxAttempts, maxTime, balancer)

	// And finally, return the ServiceMiddleware, implemented by proxymw.
	return func(next StringService) StringService {
		return proxymw{next, retry}
	}
}
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;stringsvc3&#34;&gt;stringsvc3&lt;/h3&gt;

&lt;p&gt;现在为止完整的服务是&lt;a href=&#34;https://github.com/go-kit/kit/blob/master/examples/stringsvc3&#34;&gt;stringsvc3&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;$ go get github.com/go-kit/kit/examples/stringsvc3
$ stringsvc3 -listen=:8001 &amp;amp;
listen=:8001 caller=proxying.go:25 proxy_to=none
listen=:8001 caller=main.go:72 msg=HTTP addr=:8001
$ stringsvc3 -listen=:8002 &amp;amp;
listen=:8002 caller=proxying.go:25 proxy_to=none
listen=:8002 caller=main.go:72 msg=HTTP addr=:8002
$ stringsvc3 -listen=:8003 &amp;amp;
listen=:8003 caller=proxying.go:25 proxy_to=none
listen=:8003 caller=main.go:72 msg=HTTP addr=:8003
$ stringsvc3 -listen=:8080 -proxy=localhost:8001,localhost:8002,localhost:8003
listen=:8080 caller=proxying.go:29 proxy_to=&amp;quot;[localhost:8001 localhost:8002 localhost:8003]&amp;quot;
listen=:8080 caller=main.go:72 msg=HTTP addr=:8080
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;$ for s in foo bar baz ; do curl -d&amp;quot;{\&amp;quot;s\&amp;quot;:\&amp;quot;$s\&amp;quot;}&amp;quot; localhost:8080/uppercase ; done
{&amp;quot;v&amp;quot;:&amp;quot;FOO&amp;quot;,&amp;quot;err&amp;quot;:null}
{&amp;quot;v&amp;quot;:&amp;quot;BAR&amp;quot;,&amp;quot;err&amp;quot;:null}
{&amp;quot;v&amp;quot;:&amp;quot;BAZ&amp;quot;,&amp;quot;err&amp;quot;:null}
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;listen=:8001 caller=logging.go:28 method=uppercase input=foo output=FOO err=null took=5.168µs
listen=:8080 caller=logging.go:28 method=uppercase input=foo output=FOO err=null took=4.39012ms
listen=:8002 caller=logging.go:28 method=uppercase input=bar output=BAR err=null took=5.445µs
listen=:8080 caller=logging.go:28 method=uppercase input=bar output=BAR err=null took=2.04831ms
listen=:8003 caller=logging.go:28 method=uppercase input=baz output=BAZ err=null took=3.285µs
listen=:8080 caller=logging.go:28 method=uppercase input=baz output=BAZ err=null took=1.388155ms
&lt;/code&gt;&lt;/pre&gt;
</description>
      
    </item>
    
    <item>
      <title>手动搭建kubernetes集群（四）</title>
      <link>http://anakin.github.io/post/create-k8s-4/</link>
      <pubDate>Thu, 23 May 2019 10:24:06 +0800</pubDate>
      
      <guid>http://anakin.github.io/post/create-k8s-4/</guid>
      
        <description>

&lt;p&gt;开始改造我们之前搭建的基础集群，加上安全机制。&lt;/p&gt;

&lt;h2 id=&#34;准备工作&#34;&gt;准备工作&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;1. 停止master节点的所有服务&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;查看已有的service&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get services
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;删除service（例如有一个叫xxx的service）&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl delete service xxx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;查看已有的deployments&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get deploy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;删除deployments&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl delete deploy xxx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;停止master的服务&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;service kube-calico stop
service kube-scheduler stop
service kube-controller-manager stop
service kube-apiserver stop
service etcd stop &amp;amp;&amp;amp; rm -fr /var/lib/etcd/*
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;2. 停止worker节点的所有服务&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;service kubelet stop 
rm -fr /var/lib/kubelet/*
service kube-proxy stop 
rm -fr /var/lib/kube-proxy/*
service kube-calico stop
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;3. 安装需要的工具&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;在所有节点上安装cfssl:&lt;/strong&gt;&lt;br&gt;&lt;/p&gt;

&lt;p&gt;CFSSL是CloudFlare开源的一款PKI/TLS工具。 CFSSL 包含一个命令行工具 和一个用于 签名，验证并且捆绑TLS证书的 HTTP API 服务。 用Go写的。具体相关信息自行google。
下载&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;wget -q --show-progress --https-only --timestamping \
  https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 \
  https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改权限&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;chmod +x cfssl_linux-amd64 cfssljson_linux-amd64
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;放到可执行目录&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mv cfssl_linux-amd64 /usr/local/bin/cfssl
mv cfssljson_linux-amd64 /usr/local/bin/cfssljson  
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;在worker节点上安装conntrack&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apt install conntrack
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;4.在master上生成根证书&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p /etc/kubernetes/ca
cd /etc/kubernetes/ca
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编辑ca-config.json文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;signing&amp;quot;: {
    &amp;quot;default&amp;quot;: {
      &amp;quot;expiry&amp;quot;: &amp;quot;87600h&amp;quot;
    },
    &amp;quot;profiles&amp;quot;: {
      &amp;quot;kubernetes&amp;quot;: {
        &amp;quot;usages&amp;quot;: [
            &amp;quot;signing&amp;quot;,
            &amp;quot;key encipherment&amp;quot;,
            &amp;quot;server auth&amp;quot;,
            &amp;quot;client auth&amp;quot;
        ],
        &amp;quot;expiry&amp;quot;: &amp;quot;87600h&amp;quot;
      }
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编辑ca-csr.json文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;CN&amp;quot;: &amp;quot;kubernetes&amp;quot;,
  &amp;quot;key&amp;quot;: {
    &amp;quot;algo&amp;quot;: &amp;quot;rsa&amp;quot;,
    &amp;quot;size&amp;quot;: 2048
  },
  &amp;quot;names&amp;quot;: [
    {
      &amp;quot;C&amp;quot;: &amp;quot;CN&amp;quot;,
      &amp;quot;ST&amp;quot;: &amp;quot;Beijing&amp;quot;,
      &amp;quot;L&amp;quot;: &amp;quot;XS&amp;quot;,
      &amp;quot;O&amp;quot;: &amp;quot;k8s&amp;quot;,
      &amp;quot;OU&amp;quot;: &amp;quot;System&amp;quot;
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行生成证书的命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cfssl gencert -initca ca-csr.json | cfssljson -bare ca
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;OK。&lt;/p&gt;

&lt;h2 id=&#34;master节点设置&#34;&gt;master节点设置&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;1. etcd设置&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;设置etcd的证书：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p /etc/kubernetes/ca/etcd
cd /etc/kubernetes/ca/etcd 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编辑etcd-csr.json文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;CN&amp;quot;: &amp;quot;etcd&amp;quot;,
  &amp;quot;hosts&amp;quot;: [
    &amp;quot;127.0.0.1&amp;quot;,
    &amp;quot;192.168.32.131&amp;quot;
  ],
  &amp;quot;key&amp;quot;: {
    &amp;quot;algo&amp;quot;: &amp;quot;rsa&amp;quot;,
    &amp;quot;size&amp;quot;: 2048
  },
  &amp;quot;names&amp;quot;: [
    {
      &amp;quot;C&amp;quot;: &amp;quot;CN&amp;quot;,
      &amp;quot;ST&amp;quot;: &amp;quot;Beijing&amp;quot;,
      &amp;quot;L&amp;quot;: &amp;quot;XS&amp;quot;,
      &amp;quot;O&amp;quot;: &amp;quot;k8s&amp;quot;,
      &amp;quot;OU&amp;quot;: &amp;quot;System&amp;quot;
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;执行签发证书的命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cfssl gencert \
        -ca=/etc/kubernetes/ca/ca.pem \
        -ca-key=/etc/kubernetes/ca/ca-key.pem \
        -config=/etc/kubernetes/ca/ca-config.json \
        -profile=kubernetes etcd-csr.json | cfssljson -bare etcd
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编辑/lib/systemd/system/etcd.service，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target
Documentation=https://github.com/coreos

[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
ExecStart=/home/anakin/bin/etcd \
  --name=192.168.32.131 \
  --listen-client-urls=https://192.168.32.131:2379,http://127.0.0.1:2379 \
  --advertise-client-urls=https://192.168.32.131:2379 \
  --data-dir=/var/lib/etcd \
  --listen-peer-urls=https://192.168.32.131:2380 \
  --initial-advertise-peer-urls=https://192.168.32.131:2380 \
  --cert-file=/etc/kubernetes/ca/etcd/etcd.pem \
  --key-file=/etc/kubernetes/ca/etcd/etcd-key.pem \
  --peer-cert-file=/etc/kubernetes/ca/etcd/etcd.pem \
  --peer-key-file=/etc/kubernetes/ca/etcd/etcd-key.pem \
  --trusted-ca-file=/etc/kubernetes/ca/ca.pem \
  --peer-trusted-ca-file=/etc/kubernetes/ca/ca.pem
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后启动服务：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl daemon-reload
service etcd start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;2. apiserver设置&lt;/strong&gt;&lt;br&gt;
创建一个存放证书的目录：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p /etc/kubernetes/ca/kubernetes
cd /etc/kubernetes/ca/kubernetes/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编辑kubernetes-csr.json文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;CN&amp;quot;: &amp;quot;kubernetes&amp;quot;,
  &amp;quot;hosts&amp;quot;: [
    &amp;quot;127.0.0.1&amp;quot;,
    &amp;quot;192.168.32.131&amp;quot;,
    &amp;quot;10.68.0.1&amp;quot;,
    &amp;quot;kubernetes&amp;quot;,
    &amp;quot;kubernetes.default&amp;quot;,
    &amp;quot;kubernetes.default.svc&amp;quot;,
    &amp;quot;kubernetes.default.svc.cluster&amp;quot;,
    &amp;quot;kubernetes.default.svc.cluster.local&amp;quot;
  ],
  &amp;quot;key&amp;quot;: {
    &amp;quot;algo&amp;quot;: &amp;quot;rsa&amp;quot;,
    &amp;quot;size&amp;quot;: 2048
  },
  &amp;quot;names&amp;quot;: [
    {
      &amp;quot;C&amp;quot;: &amp;quot;CN&amp;quot;,
      &amp;quot;ST&amp;quot;: &amp;quot;Beijing&amp;quot;,
      &amp;quot;L&amp;quot;: &amp;quot;XS&amp;quot;,
      &amp;quot;O&amp;quot;: &amp;quot;k8s&amp;quot;,
      &amp;quot;OU&amp;quot;: &amp;quot;System&amp;quot;
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;签发证书：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cfssl gencert \
        -ca=/etc/kubernetes/ca/ca.pem \
        -ca-key=/etc/kubernetes/ca/ca-key.pem \
        -config=/etc/kubernetes/ca/ca-config.json \
        -profile=kubernetes kubernetes-csr.json | cfssljson -bare kubernetes 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;生成一个token文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;head -c 16 /dev/urandom | od -An -t x | tr -d &#39; &#39;
c81fb8ce5502f19d510d159ff8a1cf12
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;把这个token存入文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;echo &amp;quot;c81fb8ce5502f19d510d159ff8a1cf12,kubelet-bootstrap,10001,\&amp;quot;system:kubelet-bootstrap\&amp;quot;&amp;quot; &amp;gt; /etc/kubernetes/ca/kubernetes/token.csv
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编辑/lib/systemd/system/kube-apiserver.service，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target
[Service]
ExecStart=/home/anakin/bin/kube-apiserver \
  --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota,NodeRestriction \
  --insecure-bind-address=127.0.0.1 \
  --kubelet-https=true \
  --bind-address=192.168.32.131 \
  --authorization-mode=Node,RBAC \
  --runtime-config=rbac.authorization.k8s.io/v1 \
  --enable-bootstrap-token-auth \
  --token-auth-file=/etc/kubernetes/ca/kubernetes/token.csv \
  --tls-cert-file=/etc/kubernetes/ca/kubernetes/kubernetes.pem \
  --tls-private-key-file=/etc/kubernetes/ca/kubernetes/kubernetes-key.pem \
  --client-ca-file=/etc/kubernetes/ca/ca.pem \
  --service-account-key-file=/etc/kubernetes/ca/ca-key.pem \
  --etcd-cafile=/etc/kubernetes/ca/ca.pem \
  --etcd-certfile=/etc/kubernetes/ca/kubernetes/kubernetes.pem \
  --etcd-keyfile=/etc/kubernetes/ca/kubernetes/kubernetes-key.pem \
  --service-cluster-ip-range=10.68.0.0/16 \
  --service-node-port-range=20000-40000 \
  --etcd-servers=https://192.168.32.131:2379 \
  --enable-swagger-ui=true \
  --allow-privileged=true \
  --audit-log-maxage=30 \
  --audit-log-maxbackup=3 \
  --audit-log-maxsize=100 \
  --audit-log-path=/var/lib/audit.log \
  --event-ttl=1h \
  --v=2
Restart=on-failure
RestartSec=5
Type=notify
LimitNOFILE=65536
[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后启动服务：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl daemon-reload
service kube-apiserver start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;3. controller-manager设置&lt;/strong&gt;&lt;br&gt;
编辑/lib/systemd/system/kube-controller-manager.service文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
[Service]
ExecStart=/home/anakin/bin/kube-controller-manager \
  --address=127.0.0.1 \
  --master=http://127.0.0.1:8080 \
  --allocate-node-cidrs=true \
  --service-cluster-ip-range=10.68.0.0/16 \
  --cluster-cidr=172.20.0.0/16 \
  --cluster-name=kubernetes \
  --leader-elect=true \
  --cluster-signing-cert-file=/etc/kubernetes/ca/ca.pem \
  --cluster-signing-key-file=/etc/kubernetes/ca/ca-key.pem \
  --service-account-private-key-file=/etc/kubernetes/ca/ca-key.pem \
  --root-ca-file=/etc/kubernetes/ca/ca.pem \
  --v=2
Restart=on-failure
RestartSec=5
[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动服务&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl daemon-reload
service kube-controller-manager start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;4. scheduler设置&lt;/strong&gt;&lt;br&gt;
scheduler不需要重新设置，直接启动就ok了。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;5. kubectl设置&lt;/strong&gt;&lt;br&gt;
先创建一个叫做admin的系统管理员：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p /etc/kubernetes/ca/admin
cd /etc/kubernetes/ca/admin/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编辑admin-csr.json文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;CN&amp;quot;: &amp;quot;admin&amp;quot;,
  &amp;quot;hosts&amp;quot;: [],
  &amp;quot;key&amp;quot;: {
    &amp;quot;algo&amp;quot;: &amp;quot;rsa&amp;quot;,
    &amp;quot;size&amp;quot;: 2048
  },
  &amp;quot;names&amp;quot;: [
    {
      &amp;quot;C&amp;quot;: &amp;quot;CN&amp;quot;,
      &amp;quot;ST&amp;quot;: &amp;quot;Beijing&amp;quot;,
      &amp;quot;L&amp;quot;: &amp;quot;XS&amp;quot;,
      &amp;quot;O&amp;quot;: &amp;quot;system:masters&amp;quot;,
      &amp;quot;OU&amp;quot;: &amp;quot;System&amp;quot;
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;签发证书：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cfssl gencert \
        -ca=/etc/kubernetes/ca/ca.pem \
        -ca-key=/etc/kubernetes/ca/ca-key.pem \
        -config=/etc/kubernetes/ca/ca-config.json \
        -profile=kubernetes admin-csr.json | cfssljson -bare admin
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后开始设置kubectl&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config set-cluster kubernetes \
        --certificate-authority=/etc/kubernetes/ca/ca.pem \
        --embed-certs=true \
        --server=https://192.168.32.131:6443

kubectl config set-credentials admin \
        --client-certificate=/etc/kubernetes/ca/admin/admin.pem \
        --embed-certs=true \
        --client-key=/etc/kubernetes/ca/admin/admin-key.pem

kubectl config set-context kubernetes \
        --cluster=kubernetes --user=admin
    
kubectl config use-context kubernetes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;6. calico设置&lt;/strong&gt;&lt;br&gt;
生成证书：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p /etc/kubernetes/ca/calico
cd /etc/kubernetes/ca/calico/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编辑calico-csr.json文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;CN&amp;quot;: &amp;quot;calico&amp;quot;,
  &amp;quot;hosts&amp;quot;: [],
  &amp;quot;key&amp;quot;: {
    &amp;quot;algo&amp;quot;: &amp;quot;rsa&amp;quot;,
    &amp;quot;size&amp;quot;: 2048
  },
  &amp;quot;names&amp;quot;: [
    {
      &amp;quot;C&amp;quot;: &amp;quot;CN&amp;quot;,
      &amp;quot;ST&amp;quot;: &amp;quot;Beijing&amp;quot;,
      &amp;quot;L&amp;quot;: &amp;quot;XS&amp;quot;,
      &amp;quot;O&amp;quot;: &amp;quot;k8s&amp;quot;,
      &amp;quot;OU&amp;quot;: &amp;quot;System&amp;quot;
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;签发证书：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cfssl gencert \
        -ca=/etc/kubernetes/ca/ca.pem \
        -ca-key=/etc/kubernetes/ca/ca-key.pem \
        -config=/etc/kubernetes/ca/ca-config.json \
        -profile=kubernetes calico-csr.json | cfssljson -bare calico
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编辑/lib/systemd/system/kube-calico.service文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Unit]
Description=calico node
After=docker.service
Requires=docker.service

[Service]
User=root
PermissionsStartOnly=true
ExecStart=/usr/bin/docker run --net=host --privileged --name=calico-node \
  -e NODENAME=&amp;quot;calico1&amp;quot; \
  -e ETCD_ENDPOINTS=https://192.168.32.131:2379 \
  -e ETCD_CA_CERT_FILE=/etc/kubernetes/ca/ca.pem \
  -e ETCD_CERT_FILE=/etc/kubernetes/ca/calico/calico.pem \
  -e ETCD_KEY_FILE=/etc/kubernetes/ca/calico/calico-key.pem \
  -e CALICO_LIBNETWORK_ENABLED=true \
  -e CALICO_NETWORKING_BACKEND=bird \
  -e CALICO_DISABLE_FILE_LOGGING=true \
  -e CALICO_IPV4POOL_CIDR=172.20.0.0/16 \
  -e CALICO_IPV4POOL_IPIP=off \
  -e FELIX_DEFAULTENDPOINTTOHOSTACTION=ACCEPT \
  -e FELIX_IPV6SUPPORT=false \
  -e FELIX_LOGSEVERITYSCREEN=info \
  -e FELIX_IPINIPMTU=1440 \
  -e FELIX_HEALTHENABLED=true \
  -e IP=192.168.32.131 \
  -v /etc/kubernetes/ca:/etc/kubernetes/ca \
  -v /var/run/calico:/var/run/calico \
  -v /lib/modules:/lib/modules \
  -v /run/docker/plugins:/run/docker/plugins \
  -v /var/run/docker.sock:/var/run/docker.sock \
  -v /var/log/calico:/var/log/calico \
  calico/node:release-v2.6
ExecStop=/usr/bin/docker rm -f calico-node
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动服务：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl daemon-reload
service kube-calico start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;还有一步，就是把/etc/kubernetes/ca/calico目录下的这几个证书拷贝到worker节点对应的位置上去备用&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;7. 设置kubelet的角色绑定&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl -n kube-system get clusterrole
kubectl create clusterrolebinding kubelet-bootstrap \
         --clusterrole=system:node-bootstrapper --user=kubelet-bootstrap
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;worker节点设置&#34;&gt;worker节点设置&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;1. calico设置&lt;/strong&gt;&lt;br&gt;
编辑/lib/systemd/system/kube-calico.service文件，参考master节点的内容，只需要修改一下NODENAME就可以了，然后启动服务&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2. kubelet设置&lt;/strong&gt;&lt;br&gt;
设置kubelet参数：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config set-cluster kubernetes \
        --certificate-authority=/etc/kubernetes/ca/ca.pem \
        --embed-certs=true \
        --server=https://192.168.32.131:6443 \
        --kubeconfig=bootstrap.kubeconfig

kubectl config set-credentials kubelet-bootstrap \
        --token=c81fb8ce5502f19d510d159ff8a1cf12\
        --kubeconfig=bootstrap.kubeconfig

kubectl config set-context default \
        --cluster=kubernetes \
        --user=kubelet-bootstrap \
        --kubeconfig=bootstrap.kubeconfig

kubectl config use-context default --kubeconfig=bootstrap.kubeconfig

mv bootstrap.kubeconfig /etc/kubernetes/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;设置cni：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd /etc/cni/net.d/
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编辑10-calico.conf文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    &amp;quot;name&amp;quot;: &amp;quot;calico-k8s-network&amp;quot;,
    &amp;quot;cniVersion&amp;quot;: &amp;quot;0.1.0&amp;quot;,
    &amp;quot;type&amp;quot;: &amp;quot;calico&amp;quot;,
    &amp;quot;etcd_endpoints&amp;quot;: &amp;quot;https://192.168.32.131:2379&amp;quot;,
    &amp;quot;etcd_key_file&amp;quot;: &amp;quot;/etc/kubernetes/ca/calico/calico-key.pem&amp;quot;,
    &amp;quot;etcd_cert_file&amp;quot;: &amp;quot;/etc/kubernetes/ca/calico/calico.pem&amp;quot;,
    &amp;quot;etcd_ca_cert_file&amp;quot;: &amp;quot;/etc/kubernetes/ca/ca.pem&amp;quot;,
    &amp;quot;log_level&amp;quot;: &amp;quot;info&amp;quot;,
    &amp;quot;ipam&amp;quot;: {
        &amp;quot;type&amp;quot;: &amp;quot;calico-ipam&amp;quot;
    },
    &amp;quot;kubernetes&amp;quot;: {
        &amp;quot;kubeconfig&amp;quot;: &amp;quot;/etc/kubernetes/kubelet.kubeconfig&amp;quot;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编辑/lib/systemd/system/kubelet.service文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=docker.service
Requires=docker.service

[Service]
WorkingDirectory=/var/lib/kubelet
ExecStart=/home/anakin/bin/kubelet \
  --address=192.168.32.132 \
  --hostname-override=192.168.32.132 \
  --pod-infra-container-image=registry.cn-hangzhou.aliyuncs.com/imooc/pause-amd64:3.0 \
  --kubeconfig=/etc/kubernetes/kubelet.kubeconfig \
  --experimental-bootstrap-kubeconfig=/etc/kubernetes/bootstrap.kubeconfig \
  --cert-dir=/etc/kubernetes/ca \
  --hairpin-mode hairpin-veth \
  --network-plugin=cni \
  --cni-conf-dir=/etc/cni/net.d \
  --cni-bin-dir=/home/anakin/bin \
  --cluster-dns=10.68.0.2 \
  --cluster-domain=cluster.local. \
  --allow-privileged=true \
  --fail-swap-on=false \
  --logtostderr=true \
  --v=2
#kubelet cAdvisor 默认在所有接口监听 4194 端口的请求, 以下iptables限制内网访问
ExecStartPost=/sbin/iptables -A INPUT -s 10.0.0.0/8 -p tcp --dport 4194 -j ACCEPT
ExecStartPost=/sbin/iptables -A INPUT -s 172.16.0.0/12 -p tcp --dport 4194 -j ACCEPT
ExecStartPost=/sbin/iptables -A INPUT -s 192.168.0.0/16 -p tcp --dport 4194 -j ACCEPT
ExecStartPost=/sbin/iptables -A INPUT -p tcp --dport 4194 -j DROP
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动服务：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl daemon-reload
service kubelet start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动之后，需要去master节点进行授权操作：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get csr|grep &#39;Pending&#39; | awk &#39;{print $1}&#39;| xargs kubectl certificate approve
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;strong&gt;3. kube-proxy设置&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;准备证书：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p /etc/kubernetes/ca/kube-proxy
cd /etc/kubernetes/ca/kube-proxy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编辑kube-proxy-csr.json文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
  &amp;quot;CN&amp;quot;: &amp;quot;system:kube-proxy&amp;quot;,
  &amp;quot;hosts&amp;quot;: [],
  &amp;quot;key&amp;quot;: {
    &amp;quot;algo&amp;quot;: &amp;quot;rsa&amp;quot;,
    &amp;quot;size&amp;quot;: 2048
  },
  &amp;quot;names&amp;quot;: [
    {
      &amp;quot;C&amp;quot;: &amp;quot;CN&amp;quot;,
      &amp;quot;ST&amp;quot;: &amp;quot;Beijing&amp;quot;,
      &amp;quot;L&amp;quot;: &amp;quot;XS&amp;quot;,
      &amp;quot;O&amp;quot;: &amp;quot;k8s&amp;quot;,
      &amp;quot;OU&amp;quot;: &amp;quot;System&amp;quot;
    }
  ]
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;签发证书：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cfssl gencert \
        -ca=/etc/kubernetes/ca/ca.pem \
        -ca-key=/etc/kubernetes/ca/ca-key.pem \
        -config=/etc/kubernetes/ca/ca-config.json \
        -profile=kubernetes kube-proxy-csr.json | cfssljson -bare kube-proxy
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;生成各种配置文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config set-cluster kubernetes \
        --certificate-authority=/etc/kubernetes/ca/ca.pem \
        --embed-certs=true \
        --server=https://192.168.32.131:6443 \
        --kubeconfig=kube-proxy.kubeconfig

kubectl config set-credentials kube-proxy \
        --client-certificate=/etc/kubernetes/ca/kube-proxy/kube-proxy.pem \
        --client-key=/etc/kubernetes/ca/kube-proxy/kube-proxy-key.pem \
        --embed-certs=true \
        --kubeconfig=kube-proxy.kubeconfig

kubectl config set-context default \
        --cluster=kubernetes \
        --user=kube-proxy \
        --kubeconfig=kube-proxy.kubeconfig

kubectl config use-context default --kubeconfig=kube-proxy.kubeconfig

mv kube-proxy.kubeconfig /etc/kubernetes/kube-proxy.kubeconfig
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编辑/lib/systemd/system/kube-proxy.service文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target
[Service]
WorkingDirectory=/var/lib/kube-proxy
ExecStart=/home/anakin/bin/kube-proxy \
  --bind-address=192.168.32.132 \
  --hostname-override=192.168.32.132 \
  --kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \
  --logtostderr=true \
  --v=2
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;启动服务：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;systemctl daemon-reload
service kube-proxy start
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;

&lt;p&gt;中间可能会遇到的问题：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;calico&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;calico是以docker的方式运行的，有时候重启服务的时候，之前的container还没杀死，日志里会报错误：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Error response from daemon: Conflict. The container name &amp;quot;/aaa&amp;quot; is already in use by conta
iner &amp;quot;xxx&amp;quot;.
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这时候只要删除这个container就可以了:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;docker rm -f xxx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果是报node的名字被占用的错误，到master节点删除就ok了：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;calicoctl delete node
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;一些常用的命令:&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;查看集群节点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl get nodes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;查看calico节点：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;calicoctl node status
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;每个服务启动之后，都可以用下面的命令查看服务的日志信息：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;journalctl -f -u xxx
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;ok。写的好累。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>Golang通过ETCD实现分布式锁</title>
      <link>http://anakin.github.io/post/golang-etcd-lock/</link>
      <pubDate>Wed, 22 May 2019 16:09:11 +0800</pubDate>
      
      <guid>http://anakin.github.io/post/golang-etcd-lock/</guid>
      
        <description>

&lt;p&gt;之前写过用redis实现的分布式锁，这次用etcd来实现一个。&lt;/p&gt;

&lt;h2 id=&#34;原理&#34;&gt;原理&lt;/h2&gt;

&lt;p&gt;首先获取一个etcd的租约，拿着这个租约用etcd的事务操作去设置一个key，如果设置成功，就表示抢到了锁，否则抢索失败。租约的作用就是实现抢到锁之后的释放功能，防止长期占用。&lt;br&gt;
etcd的txn事务比较特别，是一个IF-THEN-ELSE的形式，其中IF接收的参数是一个比较操作。多说无益，看代码吧。&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;代码&#34;&gt;代码&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
	&amp;quot;context&amp;quot;
	&amp;quot;fmt&amp;quot;
	&amp;quot;go.etcd.io/etcd/clientv3&amp;quot;
	&amp;quot;time&amp;quot;
)

func main() {
	config := clientv3.Config{
		Endpoints:   []string{&amp;quot;127.0.0.1:2379&amp;quot;},
		DialTimeout: 5 * time.Second,
	}
	client, err := clientv3.New(config)
	if err != nil {
		fmt.Print(err)
	}
	lease := clientv3.NewLease(client)
	leaseResp, err := lease.Grant(context.TODO(), 5)
	if err != nil {
		fmt.Println(err)
	}
	leaseID := leaseResp.ID
	ctx, cancelFunc := context.WithCancel(context.TODO())

    //两个defer用于释放锁
	defer cancelFunc()
	defer lease.Revoke(context.TODO(), leaseID)
    
    //抢锁和占用期间，需要不停的续租，续租方法返回一个只读的channel
	keepChan, err := lease.KeepAlive(ctx, leaseID)
	if err != nil {
		fmt.Println(err)
	}
    //处理续租返回的信息
	go func() {
		for {
			select {
			case keepResp := &amp;lt;-keepChan:
				if keepChan == nil {
					fmt.Println(&amp;quot;lease out&amp;quot;)
					goto END
				} else {
					fmt.Println(&amp;quot;get resp&amp;quot;, keepResp.ID)
				}
			}
		}
	END:
	}()
	kv := clientv3.NewKV(client)
	txn := kv.Txn(context.TODO())
    //开始抢锁事务操作
	txn.If(clientv3.Compare(clientv3.CreateRevision(&amp;quot;/lock/9&amp;quot;), &amp;quot;=&amp;quot;, 0)).Then(clientv3.OpPut(&amp;quot;/lock/9&amp;quot;, &amp;quot;&amp;quot;, clientv3.WithLease(leaseID))).Else(clientv3.OpGet(&amp;quot;/lock/9&amp;quot;))
    //提交事务
	txnResp, err := txn.Commit()
    
	if err != nil {
		fmt.Println(err)
		return
	}
    //如果抢锁成功
	if txnResp.Succeeded {
		fmt.Println(&amp;quot;success&amp;quot;)
	} else {
		fmt.Println(&amp;quot;fail&amp;quot;)
	}
	time.Sleep(5 * time.Second)
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      
    </item>
    
    <item>
      <title>给Mac上的Fusion虚拟机设置固定ip地址</title>
      <link>http://anakin.github.io/post/fusion-static-ip/</link>
      <pubDate>Tue, 21 May 2019 18:11:37 +0800</pubDate>
      
      <guid>http://anakin.github.io/post/fusion-static-ip/</guid>
      
        <description>&lt;p&gt;因为最近需要安装k8s的本地测试环境，所以使用Mac上的Vmware Fusion安装了几台ubuntu系统的虚拟机，某次重启的时候发现ssh登录不上去了，打开虚拟机看了一下，发现是ip地址发生了变化，研究了半天，找到了解决的方法如下：&lt;br&gt;
在Mac的Terminal上编辑下面这个文件：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo vi /Library/Preferences/VMware\ Fusion/vmnet8/dhcpd.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;最后一行是下面的内容：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; ####### VMNET DHCP Configuration. End of &amp;quot;DO NOT MODIFY SECTION&amp;quot; #######
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在这行的下面，添加虚拟机的ip信息如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;host host1 {
    hardware ethernet 00:0c:29:dd:a5:67;
    fixed-address 192.168.32.131;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;其中:&lt;br&gt;
&lt;code&gt;host1&lt;/code&gt;是在Vmware Fusion中看到的虚拟机列表里的名称；&lt;br&gt;
&lt;code&gt;00:0c:29:dd:a5:67&lt;/code&gt;是这台虚拟机的网卡MAC地址，进入虚拟机的terminal里执行&lt;code&gt;ifconfig&lt;/code&gt;就可以找到&lt;br&gt;
&lt;code&gt;192.168.32.131&lt;/code&gt;是你要设置的固定ip的地址。&lt;/p&gt;

&lt;p&gt;如果有多个虚拟主机，顺序填写就可以了。&lt;/p&gt;

&lt;p&gt;设置好之后，执行下面的刷新操作:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo /Applications/VMware\ Fusion.app/Contents/Library/vmnet-cli --configure
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，&lt;code&gt;重新启动你的Mac&lt;/code&gt;，就可以生效了。&lt;/p&gt;

&lt;p&gt;亲测。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>手动搭建kubernetes集群（三）</title>
      <link>http://anakin.github.io/post/create-k8s-3/</link>
      <pubDate>Tue, 21 May 2019 16:46:07 +0800</pubDate>
      
      <guid>http://anakin.github.io/post/create-k8s-3/</guid>
      
        <description>

&lt;p&gt;本文是这个系列的第三篇文章，前两篇记录了搭建一个k8s集群的过程，但是之前搭建好的集群少了很重要的一个部分，就是安全相关的功能，包括认证、授权等机制。&lt;br&gt;
什么是认证，什么又是授权呢，可以简单的理解为，认证的目的是知道用户是谁，授权的目的是知道用户可以做什么。先认证，知道是谁，再授权知道能做什么。&lt;br&gt;
所谓的安全，主要是针对apiserver所说的，因为k8s通过apiserver提供RESTFUL接口，所以如果有人知道你的apiserver地址，就可以修改你的集群信息了。
首先了解一下相关的基础知识，包括SSL、JWT、RBAC等等。&lt;br&gt;&lt;/p&gt;

&lt;h2 id=&#34;ssl介绍&#34;&gt;SSL介绍&lt;/h2&gt;

&lt;p&gt;SSL是一个协议，https中的s，代表的就是SSL。
网上关于SSL的介绍很多也很详细，我这里只说一下我的理解。为了保证网络传输过程中的安全，传输的信息需要进行加密处理。 加密可以分为两类，对称加密和非对称加密。
1. &lt;strong&gt;对称加密&lt;/strong&gt;&lt;br&gt;
所谓对称加密， 就是说加密和解密的方法是对称的，加密方怎么加密，解密方就怎么反过来解密。举个例子：&lt;br&gt;
Client端通过对称的加密算法md5以及一个秘钥，加密一段信息，并将加密后的信息通过请求发送给server端：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;secret = md5(key+info)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;server端为了验证请求的来源合法性，采用同样的方式重新加密，并将结果和client端发来的加密结果对比，如果一致，就认为请求是合法的。
这个过程中，双方需要持有相同的key，并使用相同的加密方法。
2. &lt;strong&gt;非对称加密&lt;/strong&gt;&lt;br&gt;
理解了对称加密，很容易想到，非对称加密就是双方的操作不一致。以常用的RSA加密来举例子：&lt;br&gt;
server端事先会生成一对key，一个可以公开的叫公钥，一个不能公开的叫私钥。公钥的内容任何人可见，但是只能用来加密，私钥用来解密（具体的原理请参考相关资料，基本思想就是质数分解）。所以上面的请求过程就变成了client端用server端给的公钥，把请求信息加密，然后发送给server，server端在收到请求后，用自己的私钥就可以解密从而获得请求信息。&lt;br&gt;
3. &lt;strong&gt;对比&lt;/strong&gt;&lt;br&gt;
使用对称加密的时候，双方都需要知道key，这就存在key被泄露的风险，而非对称加密则不存在这个问题，公钥谁都可以看，私钥不存在传输给别人的过程，安全程度大大加强。&lt;br&gt;
但是非对称加密的问题是，运算速度比较慢，效率相对比较低。&lt;br&gt;
4. &lt;strong&gt;SSL&lt;/strong&gt;&lt;br&gt;
说了这么多，终于回到SSL了，SSL大概就是结合了上面说的对称和非对称加密，利用了两者的优势，具体的操作大概是这样的：&lt;br&gt;
非对称加密不是慢么？对称加密不是容易泄露key么？那好，用非对称的方式来传输对称加密使用的key，两个问题就都解决了。大致的工作流程如下：&lt;br&gt;
+ client向server发起请求，拿到server端的公钥
+ client用公钥把自己生成的key加密，然后发送给server
+ server用私钥解密，获得了client的key
+ 两端可以愉快的用这个key通过对称加密的方式通信了。&lt;br&gt;&lt;/p&gt;

&lt;p&gt;当然实际的请求流程比我这个要复杂的多，各位自行了解吧~~&lt;/p&gt;

&lt;h2 id=&#34;jwt介绍&#34;&gt;JWT介绍&lt;/h2&gt;

&lt;p&gt;JWT的全称是json web token，是一个标准，主要用于授权和信息交换。&lt;br&gt;
看名字就知道，这货就是个token，具体来说，是一个由“.”分割的三个部分组成的字符串，这三个部分分别是：
+ header
+ playload
+ signature&lt;/p&gt;

&lt;p&gt;看起来就是这样的aaaaaa.bbbb.cccc，这个字符串本身包含了一些信息，例如可以保存用户的ID等，这样服务端在接收到token之后，通过解密就直接拿到ID，不用再去数据库里查询了。token里同时还包括使用的签名算法等。具体的使用流程就是，server收到client请求的时候，用一个自己的secret使用某种加密算法生成一个这样的Token，然后发送给client，client获得token之后，每次请求都要在Authorization header里带上获得的token，header看起来是这样的：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Authorization: Bearer &amp;lt;token&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;server端每次都会验证这个token是不是自己签发的那个有效的token，从而实现无状态http服务的状态，是不是感觉作用和session有些类似？其实还是有些差异的，比如： session存储在服务端，JWT存储在客户端。&lt;/p&gt;

&lt;h2 id=&#34;rbac介绍&#34;&gt;RBAC介绍&lt;/h2&gt;

&lt;p&gt;RBAC的全称是Role-Based Access Control，基于角色的访问控制。&lt;br&gt;
下面是我粗浅的理解：&lt;br&gt;
把系统的操作权限拆分成一个个的小单位，多个小单位赋予某个角色，然后让用户属于某个角色，这样就可以灵活的控制用户对系统的访问控制了。还是举个例子：&lt;br&gt;
某个管理后台里有很多功能，比如用户管理、订单管理、商品管理、用户留言管理，然后定义几个角色：超级管理员有所有权限，运营管理员有用户管理和留言管理权限，财务管理员有订单管理权限。ok，这样一个用户进来这个后台的时候，根据需要给他赋予某个角色，他就有了对应的管理权限，一个角色可以有多个用户，多个角色可以有相同的权限，也可以随时调整角色和权限的关系，非常灵活。不知道我说清楚了没有。具体的还请查阅相关文档。&lt;/p&gt;

&lt;h2 id=&#34;kubernetes的认证和授权&#34;&gt;kubernetes的认证和授权&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;认证&lt;/strong&gt;&lt;br&gt;
kubernetes支持三种方式的认证：&lt;br&gt;&lt;/li&gt;
&lt;li&gt;HTTPS证书认证：基于CA根证书签名的双向数字证书认证方式，用到的就是前面说的SSL；&lt;/li&gt;
&lt;li&gt;HTTP Token认证：通过一个Token来识别合法用户，可以是普通token也可以是前面说过的JWT token；&lt;/li&gt;

&lt;li&gt;&lt;p&gt;HTTP Base认证：通过用户名+密码的方式认证；&lt;br&gt;
apiserver支持设置一种或多种认证方式，如果设置了多种，那么通过其中任何一种，都认为是认证成功了。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;&lt;strong&gt;授权&lt;/strong&gt;&lt;br&gt;
apiserver支持多种授权模式，例如Node,RBAC,Webhook等，可以在apiserver启动的时候指定授权模式，同样也可以指定一种或者多种，如果指定了多种，通过其中的某一种就认为是授权成功了，和认证类似。&lt;br&gt;
客户端访问apiserver的时候，发起的http request中带有各种属性，例如user,group,path等，授权过程就是将这些属性与配置好的授权模式去比较，从而判断是否可以授权对应的操作。&lt;br&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;总结&#34;&gt;总结&lt;/h2&gt;

&lt;p&gt;絮絮叨叨的总算写完了，说的再多，都不如撸起袖子加油干，接下来就在之前搭建好的基础版集群环境里去试验一下吧~~~&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>手动搭建kubernetes集群（二）</title>
      <link>http://anakin.github.io/post/create-k8s-2/</link>
      <pubDate>Mon, 20 May 2019 15:18:19 +0800</pubDate>
      
      <guid>http://anakin.github.io/post/create-k8s-2/</guid>
      
        <description>

&lt;p&gt;根据前文准备好的环境，我们现在来一步步的搭建一个基础的k8s集群&lt;br&gt;
&lt;code&gt;注意，这里的配置信息都是按照我自己的虚拟环境来写的。&lt;/code&gt;&lt;br&gt;
把server01作为master节点，server02和server03作为worker节点&lt;br&gt;
各个节点需要配置的服务和命令如下：&lt;br&gt;
master节点上需要部署的服务包括：&lt;code&gt;etcd服务&lt;/code&gt;、&lt;code&gt;APIServer服务&lt;/code&gt;、&lt;code&gt;Scheduler服务&lt;/code&gt;、&lt;code&gt;ControllerManager服务&lt;/code&gt;、&lt;code&gt;CalicoNode服务&lt;/code&gt;、&lt;code&gt;kube-proxy服务&lt;/code&gt;、&lt;code&gt;kubectl命令&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;worker节点上需要部署的服务包括：&lt;code&gt;CalicoNode服务&lt;/code&gt;、&lt;code&gt;kubelet服务&lt;/code&gt;、&lt;code&gt;kube-proxy服务&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&#34;步骤1-准备文件&#34;&gt;步骤1：准备文件&lt;/h2&gt;

&lt;p&gt;安装k8s集群有几种方式可以选择，比如容器化的方式，比如用kubeadmin的方式，这次我们打算尝试的是使用二进制文件的方式。&lt;br&gt;
1. 登录到master虚拟机上（server01），从github上下载安装文件的压缩包，我们使用的是1.13.6版本：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    wget https://github.com/kubernetes/kubernetes/releases/download/v1.13.6/kubernetes.tar.gz
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;解压缩&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tar zxvf kubernetes.tar.gz
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;下载文件，进入刚刚解压好的文件夹&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cd kubernetes
./cluster/get-kube-binaries.sh
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这个步骤因为涉及到从官网下载文件，由于墙的原因会非常缓慢或者失败，请自行上网解决。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;步骤2-master环境部署&#34;&gt;步骤2：master环境部署&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;etcd部署：&lt;br&gt;
编写etcd服务的启动配置文件etcd.service，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Unit]
Description=Etcd Server
After=network.target
After=network-online.target
Wants=network-online.target
Documentation=https://github.com/coreos

[Service]
Type=notify
WorkingDirectory=/var/lib/etcd/
ExecStart=/home/anakin/bin/etcd \
--name=192.168.32.131 \
--listen-client-urls=http://192.168.32.131:2379,http://127.0.0.1:2379 \
--advertise-client-urls=http://192.168.32.131:2379 \
--data-dir=/var/lib/etcd
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;然后执行以下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cp etcd.service /lib/systemd/system/
systemctl enable etcd.service
mkdir -p /var/lib/etcd
service etcd start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果没有问题的话，etcd服务应该已经跑起来了。&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;APIServer部署&lt;br&gt;
编写kube-apiserver.service文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Unit]
Description=Kubernetes API Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target
[Service]
ExecStart=/home/anakin/bin/kube-apiserver \
--admission-control=NamespaceLifecycle,LimitRanger,DefaultStorageClass,ResourceQuota,NodeRestriction \
--insecure-bind-address=0.0.0.0 \
--kubelet-https=false \
--service-cluster-ip-range=10.68.0.0/16 \
--service-node-port-range=20000-40000 \
--etcd-servers=http://192.168.32.131:2379 \
--enable-swagger-ui=true \
--allow-privileged=true \
--audit-log-maxage=30 \
--audit-log-maxbackup=3 \
--audit-log-maxsize=100 \
--audit-log-path=/var/lib/audit.log \
--event-ttl=1h \
--v=2
Restart=on-failure
RestartSec=5
Type=notify
LimitNOFILE=65536
[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后执行和启动etcd类似的命令&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cp kube-apiserver.service /lib/systemd/system/
systemctl enable kube-apiserver.service
service kube-apiserver start
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;ControllerManager部署&lt;br&gt;
编写kube-controller-manager.service文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Unit]
Description=Kubernetes Controller Manager
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
[Service]
ExecStart=/home/anakin/bin/kube-controller-manager \
--address=127.0.0.1 \
--master=http://127.0.0.1:8080 \
--allocate-node-cidrs=true \
--service-cluster-ip-range=10.68.0.0/16 \
--cluster-cidr=172.20.0.0/16 \
--cluster-name=kubernetes \
--leader-elect=true \
--cluster-signing-cert-file= \
--cluster-signing-key-file= \
--v=2
Restart=on-failure
RestartSec=5
[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，同样的方式启动服务:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cp kube-controller-manager.service /lib/systemd/system/
systemctl enable kube-controller-manager.service
service kube-controller-manager start
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Scheduler部署&lt;br&gt;
编写kube-scheduler.service文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Unit]
Description=Kubernetes Scheduler
Documentation=https://github.com/GoogleCloudPlatform/kubernetes

[Service]
ExecStart=/home/anakin/bin/kube-scheduler \
--address=127.0.0.1 \
--master=http://127.0.0.1:8080 \
--leader-elect=true \
--v=2
Restart=on-failure
RestartSec=5
[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;然后，继续上面的启动方式:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cp kube-scheduler.service /lib/systemd/system/
systemctl enable kube-scheduler.service
service kube-scheduler start
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;CalicoNode部署
编写kube-calico.service文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Unit]
Description=calico node
After=docker.service
Requires=docker.service

[Service]
User=root
PermissionsStartOnly=true
ExecStart=/usr/bin/docker run --net=host --privileged --name=calico-node \
-e ETCD_ENDPOINTS=http://192.168.32.131:2379 \
-e CALICO_LIBNETWORK_ENABLED=true \
-e CALICO_NETWORKING_BACKEND=bird \
-e CALICO_DISABLE_FILE_LOGGING=true \
-e CALICO_IPV4POOL_CIDR=172.20.0.0/16 \
-e CALICO_IPV4POOL_IPIP=off \
-e FELIX_DEFAULTENDPOINTTOHOSTACTION=ACCEPT \
-e FELIX_IPV6SUPPORT=false \
-e FELIX_LOGSEVERITYSCREEN=info \
-e FELIX_IPINIPMTU=1440 \
-e FELIX_HEALTHENABLED=true \
-e IP=192.168.32.131 \
-v /var/run/calico:/var/run/calico \
-v /lib/modules:/lib/modules \
-v /run/docker/plugins:/run/docker/plugins \
-v /var/run/docker.sock:/var/run/docker.sock \
-v /var/log/calico:/var/log/calico \
registry.anakin.sun.com/k8s/calico-node:v2.6.2
ExecStop=/usr/bin/docker rm -f calico-node
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;同样的方式启动：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cp kube-calico.service /lib/systemd/system/
systemctl enable kube-calico.service
service kube-calico start
&lt;/code&gt;&lt;/pre&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;kubectl命令配置
执行以下命令(root)：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;kubectl config set-cluster kubernetes  --server=http://192.168.1.102:8080
kubectl config set-context kubernetes --cluster=kubernetes
kubectl config use-context kubernetes
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果有问题，可以手动修改配置文件：~/.kube/config&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;kube-proxy服务部署
编写kube-proxy.service文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Unit]
Description=Kubernetes Kube-Proxy Server
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=network.target
[Service]
WorkingDirectory=/var/lib/kube-proxy
ExecStart=/home/anakin/bin/kube-proxy \
--bind-address=192.168.32.131 \
--hostname-override=192.168.32.131 \
--kubeconfig=/etc/kubernetes/kube-proxy.kubeconfig \
--logtostderr=true \
--v=2
Restart=on-failure
RestartSec=5
LimitNOFILE=65536

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;编写kube-proxy.kubeconfig文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
clusters:
- cluster:
    server: http://192.168.32.131:8080
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
  name: default
current-context: default
kind: Config
preferences: {}
users: []
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后执行如下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p /var/lib/kube-proxy
cp kube-proxy.service /lib/systemd/system/
cp kube-proxy.kubeconfig /etc/kubernetes/
systemctl enable kube-proxy.service
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;OK,至此，master节点应该已经配置完成了。&lt;/p&gt;

&lt;h2 id=&#34;步骤3-worker环境部署&#34;&gt;步骤3：worker环境部署&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;CalicoNode部署&lt;br&gt;
参考master部分的内容&lt;/li&gt;

&lt;li&gt;&lt;p&gt;kube-proxy服务部署&lt;br&gt;
参考master部分的内容&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;kubelet服务配置&lt;br&gt;
编写kubelet.service文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;[Unit]
Description=Kubernetes Kubelet
Documentation=https://github.com/GoogleCloudPlatform/kubernetes
After=docker.service
Requires=docker.service

[Service]
WorkingDirectory=/var/lib/kubelet
ExecStart=/home/anakin/bin/kubelet \
--address=192.168.32.131 \
--hostname-override=192.168.32.131 \
--pod-infra-container-image=registry.anakin.sun.com/imooc/pause-amd64:3.0 \
--kubeconfig=/etc/kubernetes/kubelet.kubeconfig \
--network-plugin=cni \
--cni-conf-dir=/etc/cni/net.d \
--cni-bin-dir=/home/anakin/bin \
--cluster-dns=10.68.0.2 \
--cluster-domain=cluster.local. \
--allow-privileged=true \
--fail-swap-on=false \
--logtostderr=true \
--v=2
ExecStartPost=/sbin/iptables -A INPUT -s 10.0.0.0/8 -p tcp --dport 4194 -j ACCEPT
ExecStartPost=/sbin/iptables -A INPUT -s 172.16.0.0/12 -p tcp --dport 4194 -j ACCEPT
ExecStartPost=/sbin/iptables -A INPUT -s 192.168.0.0/16 -p tcp --dport 4194 -j ACCEPT
ExecStartPost=/sbin/iptables -A INPUT -p tcp --dport 4194 -j DROP
Restart=on-failure
RestartSec=5

[Install]
WantedBy=multi-user.target
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;编写kubelet.kubeconfig文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apiVersion: v1
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: http://192.168.32.131:8080
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: &amp;quot;&amp;quot;
  name: system:node:kube-master
current-context: system:node:kube-master
kind: Config
preferences: {}
users: []
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;编写10-calico.conf文件，内容如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;{
    &amp;quot;name&amp;quot;: &amp;quot;calico-k8s-network&amp;quot;,
    &amp;quot;cniVersion&amp;quot;: &amp;quot;0.1.0&amp;quot;,
    &amp;quot;type&amp;quot;: &amp;quot;calico&amp;quot;,
    &amp;quot;etcd_endpoints&amp;quot;: &amp;quot;http://192.168.32.131:2379&amp;quot;,
    &amp;quot;log_level&amp;quot;: &amp;quot;info&amp;quot;,
    &amp;quot;ipam&amp;quot;: {
        &amp;quot;type&amp;quot;: &amp;quot;calico-ipam&amp;quot;
    },
    &amp;quot;kubernetes&amp;quot;: {
        &amp;quot;k8s_api_root&amp;quot;: &amp;quot;http://192.168.32.131:8080&amp;quot;
    }
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，执行如下命令：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;mkdir -p /var/lib/kubelet
mkdir -p /etc/kubernetes
mkdir -p /etc/cni/net.d

cp kubelet.service /lib/systemd/system/
cp kubelet.kubeconfig /etc/kubernetes/
cp 10-calico.conf /etc/cni/net.d/

systemctl enable kubelet.service
service kubelet start
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;至此，整个集群应该已经搭建好了，如果中间遇到什么问题，可以通过查看系统日志，或者google解决。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>手动搭建kubernetes集群（一）</title>
      <link>http://anakin.github.io/post/create-k8s/</link>
      <pubDate>Mon, 20 May 2019 10:57:16 +0800</pubDate>
      
      <guid>http://anakin.github.io/post/create-k8s/</guid>
      
        <description>

&lt;p&gt;最近在看有关k8s的一些知识，想手动搭建一套环境来体验一下，本文详细记录一下搭建的过程。&lt;/p&gt;

&lt;h2 id=&#34;环境&#34;&gt;环境&lt;/h2&gt;

&lt;p&gt;三台Ubuntu系统的虚拟机，其中一台作为master，另外两台作为worker节点&lt;/p&gt;

&lt;h2 id=&#34;步骤1-安装虚拟机&#34;&gt;步骤1:安装虚拟机&lt;/h2&gt;

&lt;p&gt;我的笔记本是一台Macbook Pro，首选安装一个虚拟机软件Vmware Fusion，过程略。&lt;br&gt;
1. 下载好Ubuntu镜像，我选择的是&lt;code&gt;19.04&lt;/code&gt;版本。&lt;br&gt;
2. 打开Fusion，选择New，然后选择“Install from disk or image”,continue
3. 设置好用户名和密码，中间还可以修改使用的硬盘空间等等，这个过程就不详述了。
4. 安装好之后，进入系统，找到“terminal”，安装net-tools和ssh server
    &lt;code&gt;
    sudo apt install net-tools ssh
&lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&#34;步骤2-安装docker&#34;&gt;步骤2：安装docker&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;更新包列表：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt update
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;让apt支持https方式&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt install apt-transport-https ca-certificates curl software-properties-common
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;添加GPG密钥&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;添加docker源&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo add-apt-repository &amp;quot;deb [arch=amd64] https://download.docker.com/linux/ubuntu bionic stable&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;再次更新apt&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt update
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;确保从Docker repo安装而不是默认的Ubuntu repo&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;apt-cache policy docker-ce
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;开始安装docker&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt install docker-ce
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;接受所有ip的数据包转发&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;vi /lib/systemd/system/docker.service
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;找到ExecStart，在这上面加入下面一行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ExecStartPost=/sbin/iptables -I FORWARD -s 0.0.0.0/0 -j ACCEPT
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;重启&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo systemctl daemon-reload
sudo service docker restart
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;步骤3-系统设置&#34;&gt;步骤3：系统设置&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;关闭防火墙&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ufw disable
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;设置系统转发参数&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/sysctl.d/k8s.conf
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.conf.all.rp_filter = 0
EOF
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;sysctl -p /etc/sysctl.d/k8s.conf
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;安装ntp服务，同步时钟&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;sudo apt install ntp
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;步骤4-复制虚拟机&#34;&gt;步骤4：复制虚拟机&lt;/h2&gt;

&lt;p&gt;使用fusion的复制功能，复制出另外两台虚拟机&lt;br&gt;
关掉刚装好的虚拟机，选择Fusion菜单Virtual Machine下面的Create Full Clone，Fusion就会clone出一个一模一样的虚拟机出来，是不是很方便&lt;/p&gt;

&lt;h2 id=&#34;步骤5-设置免登陆和hosts文件&#34;&gt;步骤5：设置免登陆和hosts文件&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;打开三台虚拟机，进入terminal，执行&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ifconfig
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;找到各自的ip地址&lt;br&gt;&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;打开Mac的terminal，输入下面的命令生成ssh公钥&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;ssh-keygen -t rsa
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;将公钥分别拷贝到三台虚拟机上&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;scp .ssh/id_rsa.pub user@xxx.xxx.xxx.xxx:
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;user是虚拟机上的用户名，xxx代表各自的ip地址&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;在虚拟机上将公钥追加到.ssh/authorized_keys文件，并修改权限&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;cat id_rsa.pub &amp;gt;&amp;gt; .ssh/authorized_keys
chmod 600 authorized_keys
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;分别修改每台虚拟机上的hosts文件，用vim编辑器打开/etc/hosts,添加三台虚拟机的hosts信息，下面是我自己的host信息：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;192.168.32.131 server01
192.168.32.132 server02
192.168.32.133 server03
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;ok，到现在为止，基本的安装环境应该是准备好了。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>Etcd实现MVCC的原理</title>
      <link>http://anakin.github.io/post/etcd-mvcc/</link>
      <pubDate>Sun, 19 May 2019 22:52:01 +0800</pubDate>
      
      <guid>http://anakin.github.io/post/etcd-mvcc/</guid>
      
        <description>

&lt;p&gt;etcd满足的是CAP理论中的CP，实现了最终的强一致，使用Raft协议，Quorum机制（大多数同意原则）,&lt;/p&gt;

&lt;h2 id=&#34;mvcc的意思&#34;&gt;MVCC的意思&lt;/h2&gt;

&lt;p&gt;Multi-Version Concurrency Control 多版本并发控制，目的是为了实现并发访问&lt;/p&gt;

&lt;h2 id=&#34;实现原理&#34;&gt;实现原理&lt;/h2&gt;

&lt;p&gt;在etcd中，MVCC是如何实现的呢，先来看一下相关的代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;type revision struct {
main int64
sub int64
} 

type generation struct {
ver     int64
created revision 
revs    []revision
} 

type keyIndex struct {
key         []byte
modified    revision 
generations []generation
} 
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;根据代码，可以看出：
* 每个tx事务有唯一事务ID，在etcd中叫做main ID，全局递增不重复。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;一个tx可以包含多个修改操作（put和delete），每一个操作叫做一个revision（修订），共享同一个main ID。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;一个tx内连续的多个修改操作会被从0递增编号，这个编号叫做sub ID。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;每个revision由（main ID，sub ID）唯一标识。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;多个版本的修改历史，保存在generations中&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;每一次操作行为都被单独记录下来，用户value保存到bbolt中。&lt;/p&gt;

&lt;p&gt;在bbolt中，每个revision将作为key，即序列化（revision.main+revision.sub）作为key。因此，我们先通过内存btree在keyIndex.generations[0].revs中找到最后一条revision，即可去bbolt中读取对应的数据。&lt;/p&gt;

&lt;p&gt;相应的，etcd支持按key前缀查询，其实也就是遍历btree的同时根据revision去bbolt中获取用户的value。&lt;/p&gt;

&lt;p&gt;总结一下就是，内存btree维护的是用户key =&amp;gt; keyIndex的映射，keyIndex内维护多版本的revision信息，而revision可以映射到磁盘bbolt中的用户value。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>Redis持久化</title>
      <link>http://anakin.github.io/post/redis-rdb-aof/</link>
      <pubDate>Fri, 17 May 2019 13:22:22 +0800</pubDate>
      
      <guid>http://anakin.github.io/post/redis-rdb-aof/</guid>
      
        <description>

&lt;p&gt;redis的持久化有两种方式，RDB和AOF
 ## RDB:
在指定的时间间隔内，执行指定次数的写操作，则会将内存中的数据写入到磁盘中。即在指定目录下生成一个dump.rdb文件。Redis 重启会通过加载dump.rdb文件恢复数据。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;配置方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;save 900 1
save 300 10
save 60 10000
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;意思是， 900秒内有1个更改，300秒内有10个更改以及60秒内有10000个更改，则将内存中的数据快照写入磁盘。&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;恢复方法&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;将dump.rdb 文件拷贝到redis的安装目录的bin目录下，重启redis服务即可&lt;/p&gt;

&lt;h2 id=&#34;aof&#34;&gt;AOF&lt;/h2&gt;

&lt;p&gt;采用日志的形式来记录每个写操作，并追加到文件中。Redis 重启的会根据日志文件的内容将写指令从前到后执行一次以完成数据的恢复工作。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;配置方法&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;appendonly yes
appendfilename &amp;quot;appendonly.aof&amp;quot;
#指定更新条件
# appendfsync always
appendfsync everysec
# appendfsync no
#配置重写触发机制
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;恢复方法&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;将appendonly.aof 文件拷贝到redis的安装目录的bin目录下，重启redis服务即可。如果因为某些原因导致appendonly.aof 文件格式异常，从而导致数据还原失败，可以通过命令redis-check-aof &amp;ndash;fix appendonly.aof 进行修复&lt;/p&gt;

&lt;h2 id=&#34;区别&#34;&gt;区别&lt;/h2&gt;

&lt;p&gt;RDB通过fork的方式进行处理，性能更好
AOF备份所有的操作，数据更完整，但是效率略差，文件相对较大
实际环境中，可以两者同时使用&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>Golang利用redis实现分布式锁</title>
      <link>http://anakin.github.io/post/golang-redis-lock/</link>
      <pubDate>Thu, 16 May 2019 20:11:09 +0800</pubDate>
      
      <guid>http://anakin.github.io/post/golang-redis-lock/</guid>
      
        <description>

&lt;h2 id=&#34;原理&#34;&gt;原理&lt;/h2&gt;

&lt;p&gt;使用SETNX命令(SET if Not eXists)&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;SETNX key value
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;将 key 的值设为 value，当且仅当 key 不存在。若给定的 key 已经存在，则 SETNX 不做任何动作。&lt;/p&gt;

&lt;p&gt;设置成功，返回 1 。&lt;/p&gt;

&lt;p&gt;设置失败，返回 0 。&lt;/p&gt;

&lt;p&gt;为防止获取锁之后，忘记删除，成功后再设置一个过期时间&lt;/p&gt;

&lt;p&gt;以上就是利用redis实现分布式锁的原理&lt;/p&gt;

&lt;h2 id=&#34;代码&#34;&gt;代码&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;package main

import (
	&amp;quot;fmt&amp;quot;
	&amp;quot;github.com/gomodule/redigo/redis&amp;quot;
	&amp;quot;log&amp;quot;
	&amp;quot;time&amp;quot;
)

type Lock struct {
	resource string
	token    string
	conn     redis.Conn
	timeout  int
}

func (lock *Lock) tryLock() (ok bool, err error) {
	_, err = redis.String(lock.conn.Do(&amp;quot;SET&amp;quot;, lock.key(), lock.token, &amp;quot;EX&amp;quot;, int(lock.timeout), &amp;quot;NX&amp;quot;))
	if err == redis.ErrNil {
		return false, nil
	}
	if err != nil {
		return false, err
	}
	return true, nil
}
func (lock *Lock) Unlock() (err error) {
	_, err = lock.conn.Do(&amp;quot;del&amp;quot;, lock.key())
	return
}

func (lock *Lock) key() string {
	return fmt.Sprintf(&amp;quot;redislock:%s&amp;quot;, lock.resource)
}

func (lock *Lock) AddTimeout(ex_time int64) (ok bool, err error) {
	ttl_time, err := redis.Int64(lock.conn.Do(&amp;quot;TTL&amp;quot;, lock.key()))
	fmt.Println(ttl_time)
	if err != nil {
		log.Fatal(err)
	}
	if ttl_time &amp;gt; 0 {
		fmt.Println(11)
		_, err := redis.String(lock.conn.Do(&amp;quot;SET&amp;quot;, lock.key(), lock.token, &amp;quot;EX&amp;quot;, int(ttl_time+ex_time)))
		if err == redis.ErrNil {
			return false, nil
		}
		if err != nil {
			return false, err
		}
	}
	return false, nil
}

func TryLock(conn redis.Conn, resource string, token string, DefaultTimeout int) (lock *Lock, ok bool, err error) {
	return TryLockWithTimeout(conn, resource, token, DefaultTimeout)
}

func TryLockWithTimeout(conn redis.Conn, resource string, token string, timeout int) (lock *Lock, ok bool, err error) {
	lock = &amp;amp;Lock{resource: resource, token: token, conn: conn, timeout: timeout}
	ok, err = lock.tryLock()
	if !ok || err != nil {
		lock = nil
	}
	return
}

func main() {
	fmt.Println(&amp;quot;start&amp;quot;)
	DefaultTimeout := 10
	conn, err := redis.Dial(&amp;quot;tcp&amp;quot;, &amp;quot;localhost:6379&amp;quot;)
	if err != nil {
		log.Fatal(err)
	}
	lock, ok, err := TryLock(conn, &amp;quot;anakin.sun&amp;quot;, &amp;quot;token&amp;quot;, int(DefaultTimeout))
	if err != nil {
		log.Fatal(&amp;quot;error lock&amp;quot;)
	}
	if !ok {
		log.Fatal(&amp;quot;lock fail&amp;quot;)
	}
	lock.AddTimeout(100)
	time.Sleep(time.Duration(DefaultTimeout) * time.Second)
	fmt.Println(&amp;quot;end&amp;quot;)
	defer lock.Unlock()
}
&lt;/code&gt;&lt;/pre&gt;
</description>
      
    </item>
    
    <item>
      <title>TCP协议细节学习</title>
      <link>http://anakin.github.io/post/tcp-detail/</link>
      <pubDate>Tue, 14 May 2019 12:34:43 +0800</pubDate>
      
      <guid>http://anakin.github.io/post/tcp-detail/</guid>
      
        <description>

&lt;h2 id=&#34;tcp协议中包含ip信息么&#34;&gt;TCP协议中包含ip信息么&lt;/h2&gt;

&lt;p&gt;TCP协议中并不包含ip信息，ip信息是在第三层处理的，TCP中处理的是端口信息&lt;/p&gt;

&lt;h2 id=&#34;mss的值是如何计算的&#34;&gt;MSS的值是如何计算的&lt;/h2&gt;

&lt;p&gt;TCP协议中可选的MSS（Maximum Segment Size，最大报文长度））参数，一般使用MTU代替，值为1460。这个值是怎么来的呢？
Maximum Transmission Unit，缩写MTU，中文名是：最大传输单元。
假设MTU值和IP数据包大小一致，一个IP数据包的大小是：65535，那么加上以太网帧头和为，一个以太网帧的大小就是：65535 + 14 + 4 = 65553，看起来似乎很完美，发送方也不需要拆包，接收方也不需要重组。
那么假设我们现在的带宽是：100Mbps，因为以太网帧是传输中的最小可识别单元，再往下就是0101所对应的光信号了，所以我们的一条带宽同时只能发送一个以太网帧。如果同时发送多个，那么对端就无法重组成一个以太网帧了，在100Mbps的带宽中（假设中间没有损耗），我们计算一下发送这一帧需要的时间：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;( 65553 * 8 ) / ( 100 * 1024 * 1024 ) ≈ 0.005(s)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在100M网络下传输一帧就需要5ms，也就是说这5ms其他进程发送不了任何数据。如果是早先的电话拨号，网速只有2M的情况下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;( 65553 * 8 ) / ( 2 * 1024 * 1024 ) ≈ 0.100(s)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;100ms，这简直是噩梦。其实这就像红绿灯，时间要设置合理，交替通行，不然同一个方向如果一直是绿灯，那么另一个方向就要堵成翔了。
既然大了不行，那设置小一点可以么？
假设MTU值设置为100，那么单个帧传输的时间，在2Mbps带宽下需要：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;( 100 * 8 ) / ( 2 * 1024 * 1024 ) * 1000 ≈ 5(ms)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;时间上已经能接受了，问题在于，不管MTU设置为多少，以太网头帧尾大小是固定的，都是14 + 4，所以在MTU为100的时候，一个以太网帧的传输效率为：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;( 100 - 14 - 4 ) / 100 = 82%
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;写成公式就是：( T - 14 - 4 ) / T，当T趋于无穷大的时候，效率接近100%，也就是MTU的值越大，传输效率最高，但是基于上一点传输时间的问题，来个折中的选择吧，既然头加尾是18，那就凑个整来个1500，总大小就是1518，传输效率：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;1500 / 1518 =  98.8%
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;100Mbps传输时间：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;( 1518 * 8 ) / ( 100 * 1024 * 1024 ) * 1000 = 0.11(ms)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;2Mbps传输时间：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;( 1518 * 8 ) / ( 2 * 1024 * 1024 ) * 1000 = 5.79(ms)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;总体上时间都还能接受
因此，1500，是一个折中的结果而已，这就是为啥路由器上一般都设置成这个值。
另外，如果使用PPPoE协议（ADSL）,就需要设置成更小的值，为啥呢，。
PPPoE协议头信息为:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;| VER(4bit) | TYPE(4bit) | CODE(8bit) | SESSION-ID(16bit) | LENGTH(16bit) |
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里总共是48位，也就是6个字节，那么另外2个字节是什么呢？答案是PPP协议的ID号，占用两个字节，所以在PPPoE环境下，最佳MTU值应该是：1500 - 4 - 2 = 1492
说回来，MTU的值的计算，需要从1500中减去IP数据包包头的大小20Bytes和TCP数据段的包头20Bytes，最后就得到了1460。&lt;/p&gt;

&lt;h2 id=&#34;四次挥手的原因&#34;&gt;四次挥手的原因&lt;/h2&gt;

&lt;p&gt;TCP连接是全双工的，即一端接收到FIN报时，对端虽然不再能发送数据，但是可以接收数据，所以需要两边都关闭连接才算完全关闭了这条TCP连接。&lt;/p&gt;

&lt;h2 id=&#34;time-wait状态&#34;&gt;TIME-WAIT状态&lt;/h2&gt;

&lt;p&gt;主动关闭的一方收到对端发出的FIN报之后，就从FIN-WAIT-2状态切换到TIME-WAIT状态了，再等待2MSL时间才再切换到CLOSED状态。这么做的原因在于：&lt;/p&gt;

&lt;p&gt;确保被动关闭的一方有足够的时间收到ACK，如果没有收到会触发重传。
有足够的时间，以让该连接不会与后面的连接混在一起。
TIME-WAIT状态如果过多，会占用系统资源。Linux下有几个参数可以调整TIME-WAIT状态时间：&lt;/p&gt;

&lt;p&gt;net.ipv4.tcp_tw_reuse = 1 表示开启重用。允许将TIME-WAIT sockets重新用于新的TCP连接，默认为0，表示关闭。&lt;/p&gt;

&lt;p&gt;net.ipv4.tcp_tw_recycle = 1 表示开启TCP连接中TIME-WAIT sockets的快速回收，默认为0，表示关闭。&lt;/p&gt;

&lt;p&gt;net.ipv4.tcp_max_tw_buckets = 5000表示系统同时保持TIME_WAIT套接字的最大数量，如果超过这个数字，TIME_WAIT套接字将立刻被清除并打印警告信息。默认为180000，改为5000。&lt;/p&gt;

&lt;p&gt;然而，从TCP状态转换图可以看出，主动进行关闭的链接才会进入TIME-WAIT状态，所以最好的办法：尽量不要让服务器主动关闭链接，除非一些异常情况，如客户端协议错误、客户端超时等等。&lt;/p&gt;
</description>
      
    </item>
    
    <item>
      <title>Golang的GC学习</title>
      <link>http://anakin.github.io/post/golang-gc/</link>
      <pubDate>Sun, 12 May 2019 23:05:48 +0800</pubDate>
      
      <guid>http://anakin.github.io/post/golang-gc/</guid>
      
        <description>

&lt;h2 id=&#34;stw触发的时间&#34;&gt;STW触发的时间&lt;/h2&gt;

&lt;p&gt;一次GC有两次触发STW，一次是GC的开始阶段，主要是开启写屏障和辅助GC等操作
另外就是表记完成之后，重新扫描部分根对象，禁用写屏障&lt;/p&gt;

&lt;h2 id=&#34;gc的触发条件&#34;&gt;GC的触发条件&lt;/h2&gt;

&lt;p&gt;GC在满足一定条件后会被触发, 触发条件有以下几种:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;gcTriggerAlways: 强制触发GC&lt;/li&gt;
&lt;li&gt;gcTriggerHeap: 当前分配的内存达到一定值就触发GC&lt;/li&gt;
&lt;li&gt;gcTriggerTime: 当一定时间没有执行过GC就触发GC&lt;/li&gt;
&lt;li&gt;gcTriggerCycle: 要求启动新一轮的GC, 已启动则跳过, 手动触发GC的runtime.GC()会使用这个条件&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;三色标记的过程&#34;&gt;三色标记的过程&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;所有对象最开始都是白色。&lt;/li&gt;
&lt;li&gt;从 root 开始找到所有可达对象，标记为灰色，放入待处理队列。&lt;/li&gt;
&lt;li&gt;遍历灰色对象队列，将其引用对象标记为灰色放入待处理队列，自身标记为黑色。&lt;/li&gt;
&lt;li&gt;处理完灰色对象队列，执行清扫工作。&lt;/li&gt;
&lt;/ul&gt;
</description>
      
    </item>
    
  </channel>
</rss>
